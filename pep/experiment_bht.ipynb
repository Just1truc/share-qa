{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import random\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Binary Tree Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 14.052316665649414\n",
      "Loss: 16.453035354614258\n",
      "Loss: 15.346094131469727\n",
      "Loss: 13.340925216674805\n",
      "Loss: 13.54625129699707\n",
      "Loss: 12.48184585571289\n",
      "Loss: 15.070412635803223\n",
      "Loss: 11.933829307556152\n",
      "Loss: 15.686492919921875\n",
      "Loss: 14.974651336669922\n",
      "Loss: 13.273134231567383\n",
      "Loss: 12.10181713104248\n",
      "Loss: 12.965959548950195\n",
      "Loss: 16.144683837890625\n",
      "Loss: 14.920731544494629\n",
      "Loss: 16.661026000976562\n",
      "Loss: 11.881268501281738\n",
      "Loss: 15.943611145019531\n",
      "Loss: 15.138113975524902\n",
      "Loss: 11.625199317932129\n",
      "Loss: 14.276535034179688\n",
      "Loss: 13.001980781555176\n",
      "Loss: 12.072905540466309\n",
      "Loss: 19.355669021606445\n",
      "Loss: 12.75156021118164\n",
      "Loss: 13.264352798461914\n",
      "Loss: 15.138164520263672\n",
      "Loss: 13.575176239013672\n",
      "Loss: 13.412993431091309\n",
      "Loss: 12.051664352416992\n",
      "Loss: 16.39854621887207\n",
      "Loss: 15.619536399841309\n",
      "Loss: 13.621010780334473\n",
      "Loss: 14.802406311035156\n",
      "Loss: 13.913017272949219\n",
      "Loss: 12.637627601623535\n",
      "Loss: 13.242046356201172\n",
      "Loss: 15.567933082580566\n",
      "Loss: 13.310282707214355\n",
      "Loss: 13.25949478149414\n",
      "Loss: 12.78539752960205\n",
      "Loss: 15.257455825805664\n",
      "Loss: 13.373857498168945\n",
      "Loss: 12.448150634765625\n",
      "Loss: 14.976640701293945\n",
      "Loss: 14.171272277832031\n",
      "Loss: 21.910064697265625\n",
      "Loss: 13.234546661376953\n",
      "Loss: 11.718509674072266\n",
      "Loss: 11.99896240234375\n",
      "Loss: 11.636248588562012\n",
      "Loss: 16.011638641357422\n",
      "Loss: 12.014378547668457\n",
      "Loss: 18.702932357788086\n",
      "Loss: 11.872282028198242\n",
      "Loss: 13.106882095336914\n",
      "Loss: 18.560606002807617\n",
      "Loss: 11.767332077026367\n",
      "Loss: 14.94549560546875\n",
      "Loss: 14.039090156555176\n",
      "Loss: 14.049569129943848\n",
      "Loss: 14.658513069152832\n",
      "Loss: 15.832725524902344\n",
      "Loss: 15.539895057678223\n",
      "Loss: 20.42647933959961\n",
      "Loss: 13.413607597351074\n",
      "Loss: 12.3065767288208\n",
      "Loss: 13.019661903381348\n",
      "Loss: 12.732209205627441\n",
      "Loss: 13.05224895477295\n",
      "Loss: 13.4255952835083\n",
      "Loss: 15.103254318237305\n",
      "Loss: 18.361072540283203\n",
      "Loss: 15.56297492980957\n",
      "Loss: 12.585294723510742\n",
      "Loss: 15.697675704956055\n",
      "Loss: 13.791043281555176\n",
      "Loss: 13.881105422973633\n",
      "Loss: 14.143919944763184\n",
      "Loss: 13.889398574829102\n",
      "Loss: 12.255087852478027\n",
      "Loss: 14.13818359375\n",
      "Loss: 13.607183456420898\n",
      "Loss: 14.395829200744629\n",
      "Loss: 12.252394676208496\n",
      "Loss: 13.053339004516602\n",
      "Loss: 15.944747924804688\n",
      "Loss: 19.446094512939453\n",
      "Loss: 14.070770263671875\n",
      "Loss: 14.364084243774414\n",
      "Loss: 15.083483695983887\n",
      "Loss: 13.211860656738281\n",
      "Loss: 13.505729675292969\n",
      "Loss: 13.256957054138184\n",
      "Loss: 13.939725875854492\n",
      "Loss: 18.37158966064453\n",
      "Loss: 15.808566093444824\n",
      "Loss: 13.98581314086914\n",
      "Loss: 13.271768569946289\n",
      "Loss: 14.018848419189453\n",
      "Loss: 16.14739227294922\n",
      "Loss: 12.199637413024902\n",
      "Loss: 15.140201568603516\n",
      "Loss: 12.002833366394043\n",
      "Loss: 12.99592399597168\n",
      "Loss: 18.362056732177734\n",
      "Loss: 13.020585060119629\n",
      "Loss: 15.664644241333008\n",
      "Loss: 15.615663528442383\n",
      "Loss: 15.490678787231445\n",
      "Loss: 12.64028263092041\n",
      "Loss: 12.769278526306152\n",
      "Loss: 12.843978881835938\n",
      "Loss: 12.82292366027832\n",
      "Loss: 15.171724319458008\n",
      "Loss: 13.222148895263672\n",
      "Loss: 12.383389472961426\n",
      "Loss: 14.351621627807617\n",
      "Loss: 12.861640930175781\n",
      "Loss: 12.300540924072266\n",
      "Loss: 15.245567321777344\n",
      "Loss: 12.442041397094727\n",
      "Loss: 12.56019401550293\n",
      "Loss: 15.750533103942871\n",
      "Loss: 12.50095272064209\n",
      "Loss: 15.023405075073242\n",
      "Loss: 15.58720588684082\n",
      "Loss: 13.46055793762207\n",
      "Loss: 12.472698211669922\n",
      "Loss: 12.083717346191406\n",
      "Loss: 15.322305679321289\n",
      "Loss: 12.361684799194336\n",
      "Loss: 14.262802124023438\n",
      "Loss: 12.065403938293457\n",
      "Loss: 12.58629035949707\n",
      "Loss: 17.147876739501953\n",
      "Loss: 16.575349807739258\n",
      "Loss: 13.117786407470703\n",
      "Loss: 11.583555221557617\n",
      "Loss: 20.325897216796875\n",
      "Loss: 13.632948875427246\n",
      "Loss: 12.926088333129883\n",
      "Loss: 11.543935775756836\n",
      "Loss: 15.425628662109375\n",
      "Loss: 13.311079025268555\n",
      "Loss: 15.536727905273438\n",
      "Loss: 14.667722702026367\n",
      "Loss: 12.791854858398438\n",
      "Loss: 12.90881633758545\n",
      "Loss: 19.45712661743164\n",
      "Loss: 14.160995483398438\n",
      "Loss: 20.38707733154297\n",
      "Loss: 17.829891204833984\n",
      "Loss: 16.886371612548828\n",
      "Loss: 15.209447860717773\n",
      "Loss: 11.93754768371582\n",
      "Loss: 11.96617317199707\n",
      "Loss: 11.62722396850586\n",
      "Loss: 13.909170150756836\n",
      "Loss: 19.965343475341797\n",
      "Loss: 17.67529296875\n",
      "Loss: 11.872995376586914\n",
      "Loss: 17.733684539794922\n",
      "Loss: 14.273802757263184\n",
      "Loss: 20.423730850219727\n",
      "Loss: 14.145910263061523\n",
      "Loss: 14.004565238952637\n",
      "Loss: 13.299300193786621\n",
      "Loss: 12.990010261535645\n",
      "Loss: 12.709854125976562\n",
      "Loss: 18.877304077148438\n",
      "Loss: 17.781368255615234\n",
      "Loss: 23.03132438659668\n",
      "Loss: 12.30078411102295\n",
      "Loss: 13.436880111694336\n",
      "Loss: 11.585503578186035\n",
      "Loss: 17.163663864135742\n",
      "Loss: 16.848430633544922\n",
      "Loss: 12.141019821166992\n",
      "Loss: 16.645484924316406\n",
      "Loss: 13.117063522338867\n",
      "Loss: 14.164228439331055\n",
      "Loss: 15.799860000610352\n",
      "Loss: 13.047666549682617\n",
      "Loss: 14.962581634521484\n",
      "Loss: 12.592442512512207\n",
      "Loss: 14.00930404663086\n",
      "Loss: 14.509419441223145\n",
      "Loss: 13.904510498046875\n",
      "Loss: 30.412872314453125\n",
      "Loss: 12.86171817779541\n",
      "Loss: 16.67586898803711\n",
      "Loss: 12.175313949584961\n",
      "Loss: 16.289230346679688\n",
      "Loss: 12.727460861206055\n",
      "Loss: 13.477785110473633\n",
      "Loss: 19.385852813720703\n",
      "Loss: 12.136116981506348\n",
      "Loss: 12.334813117980957\n",
      "Loss: 16.90015411376953\n",
      "Loss: 17.31653594970703\n",
      "Loss: 12.771697998046875\n",
      "Loss: 18.3693904876709\n",
      "Loss: 13.362502098083496\n",
      "Loss: 11.781347274780273\n",
      "Loss: 14.969539642333984\n",
      "Loss: 13.889494895935059\n",
      "Loss: 16.029619216918945\n",
      "Loss: 11.8596830368042\n",
      "Loss: 11.986129760742188\n",
      "Loss: 12.980022430419922\n",
      "Loss: 12.24406623840332\n",
      "Loss: 16.343652725219727\n",
      "Loss: 12.759245872497559\n",
      "Loss: 13.359196662902832\n",
      "Loss: 12.036284446716309\n",
      "Loss: 14.630659103393555\n",
      "Loss: 13.699237823486328\n",
      "Loss: 11.8300142288208\n",
      "Loss: 16.700016021728516\n",
      "Loss: 15.789085388183594\n",
      "Loss: 14.557474136352539\n",
      "Loss: 12.169315338134766\n",
      "Loss: 16.41965103149414\n",
      "Loss: 12.97464370727539\n",
      "Loss: 14.326263427734375\n",
      "Loss: 15.074783325195312\n",
      "Loss: 15.69612979888916\n",
      "Loss: 13.351964950561523\n",
      "Loss: 12.068314552307129\n",
      "Loss: 16.676719665527344\n",
      "Loss: 12.789682388305664\n",
      "Loss: 13.863048553466797\n",
      "Loss: 12.84282112121582\n",
      "Loss: 17.497814178466797\n",
      "Loss: 12.804353713989258\n",
      "Loss: 12.211570739746094\n",
      "Loss: 12.570369720458984\n",
      "Loss: 12.894475936889648\n",
      "Loss: 12.020007133483887\n",
      "Loss: 12.089635848999023\n",
      "Loss: 18.378559112548828\n",
      "Loss: 11.984503746032715\n",
      "Loss: 12.31440544128418\n",
      "Loss: 18.911148071289062\n",
      "Loss: 14.649186134338379\n",
      "Loss: 17.527565002441406\n",
      "Loss: 12.132585525512695\n",
      "Loss: 11.955690383911133\n",
      "Loss: 16.192535400390625\n",
      "Loss: 12.45261001586914\n",
      "Loss: 20.439983367919922\n",
      "Loss: 12.327215194702148\n",
      "Loss: 13.29566478729248\n",
      "Loss: 12.14268684387207\n",
      "Loss: 12.16624927520752\n",
      "Loss: 14.238666534423828\n",
      "Loss: 12.2090425491333\n",
      "Loss: 11.906899452209473\n",
      "Loss: 12.261409759521484\n",
      "Loss: 12.377272605895996\n",
      "Loss: 12.862038612365723\n",
      "Loss: 12.895883560180664\n",
      "Loss: 13.801535606384277\n",
      "Loss: 16.041942596435547\n",
      "Loss: 11.642226219177246\n",
      "Loss: 13.04428768157959\n",
      "Loss: 11.877504348754883\n",
      "Loss: 15.850455284118652\n",
      "Loss: 12.370266914367676\n",
      "Loss: 12.471821784973145\n",
      "Loss: 15.658437728881836\n",
      "Loss: 12.184800148010254\n",
      "Loss: 15.449478149414062\n",
      "Loss: 15.321758270263672\n",
      "Loss: 11.862286567687988\n",
      "Loss: 14.36817741394043\n",
      "Loss: 15.174505233764648\n",
      "Loss: 13.096303939819336\n",
      "Loss: 11.767678260803223\n",
      "Loss: 11.816618919372559\n",
      "Loss: 13.613293647766113\n",
      "Loss: 14.236180305480957\n",
      "Loss: 15.63963508605957\n",
      "Loss: 13.08938217163086\n",
      "Loss: 12.94622802734375\n",
      "Loss: 15.723607063293457\n",
      "Loss: 11.843170166015625\n",
      "Loss: 12.845813751220703\n",
      "Loss: 12.406021118164062\n",
      "Loss: 14.690007209777832\n",
      "Loss: 12.405447006225586\n",
      "Loss: 21.277925491333008\n",
      "Loss: 17.736936569213867\n",
      "Loss: 13.730199813842773\n",
      "Loss: 14.295254707336426\n",
      "Loss: 17.78787612915039\n",
      "Loss: 18.810789108276367\n",
      "Loss: 11.49146842956543\n",
      "Loss: 16.79037094116211\n",
      "Loss: 14.327081680297852\n",
      "Loss: 14.814538955688477\n",
      "Loss: 13.573702812194824\n",
      "Loss: 12.579322814941406\n",
      "Loss: 14.889811515808105\n",
      "Loss: 14.333048820495605\n",
      "Loss: 14.665894508361816\n",
      "Loss: 15.166805267333984\n",
      "Loss: 11.78589916229248\n",
      "Loss: 12.388191223144531\n",
      "Loss: 15.72552490234375\n",
      "Loss: 11.78715705871582\n",
      "Loss: 15.65167236328125\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 290\u001b[39m\n\u001b[32m    287\u001b[39m attention_mask = batch[\u001b[33m\"\u001b[39m\u001b[33mattention_mask\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    289\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m--> \u001b[39m\u001b[32m290\u001b[39m     x = \u001b[43mbert\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m)\u001b[49m.last_hidden_state\n\u001b[32m    291\u001b[39m     teacher_attn = teacher(x)\n\u001b[32m    292\u001b[39m     topk = torch.topk(teacher_attn[:, \u001b[32m0\u001b[39m], k=\u001b[32m1\u001b[39m, dim=-\u001b[32m1\u001b[39m).indices.squeeze(-\u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.13/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.13/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.13/site-packages/transformers/models/bert/modeling_bert.py:1142\u001b[39m, in \u001b[36mBertModel.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m   1135\u001b[39m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[32m   1136\u001b[39m \u001b[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[32m   1137\u001b[39m \u001b[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[32m   1138\u001b[39m \u001b[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[32m   1139\u001b[39m \u001b[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[32m   1140\u001b[39m head_mask = \u001b[38;5;28mself\u001b[39m.get_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m.config.num_hidden_layers)\n\u001b[32m-> \u001b[39m\u001b[32m1142\u001b[39m encoder_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1143\u001b[39m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1144\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1145\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1146\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1147\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1148\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1149\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1150\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1151\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1152\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1153\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1154\u001b[39m sequence_output = encoder_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m   1155\u001b[39m pooled_output = \u001b[38;5;28mself\u001b[39m.pooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.pooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.13/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.13/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.13/site-packages/transformers/models/bert/modeling_bert.py:695\u001b[39m, in \u001b[36mBertEncoder.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m    684\u001b[39m     layer_outputs = \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(\n\u001b[32m    685\u001b[39m         layer_module.\u001b[34m__call__\u001b[39m,\n\u001b[32m    686\u001b[39m         hidden_states,\n\u001b[32m   (...)\u001b[39m\u001b[32m    692\u001b[39m         output_attentions,\n\u001b[32m    693\u001b[39m     )\n\u001b[32m    694\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m695\u001b[39m     layer_outputs = \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    696\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    697\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    698\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    699\u001b[39m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    700\u001b[39m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    701\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    702\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    703\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    705\u001b[39m hidden_states = layer_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    706\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.13/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.13/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.13/site-packages/transformers/models/bert/modeling_bert.py:585\u001b[39m, in \u001b[36mBertLayer.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[39m\n\u001b[32m    573\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34mforward\u001b[39m(\n\u001b[32m    574\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    575\u001b[39m     hidden_states: torch.Tensor,\n\u001b[32m   (...)\u001b[39m\u001b[32m    582\u001b[39m ) -> Tuple[torch.Tensor]:\n\u001b[32m    583\u001b[39m     \u001b[38;5;66;03m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[32m    584\u001b[39m     self_attn_past_key_value = past_key_value[:\u001b[32m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m585\u001b[39m     self_attention_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    586\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    587\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    588\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    589\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    590\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m=\u001b[49m\u001b[43mself_attn_past_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    591\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    592\u001b[39m     attention_output = self_attention_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    594\u001b[39m     \u001b[38;5;66;03m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.13/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.13/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.13/site-packages/transformers/models/bert/modeling_bert.py:524\u001b[39m, in \u001b[36mBertAttention.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[39m\n\u001b[32m    505\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34mforward\u001b[39m(\n\u001b[32m    506\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    507\u001b[39m     hidden_states: torch.Tensor,\n\u001b[32m   (...)\u001b[39m\u001b[32m    513\u001b[39m     output_attentions: Optional[\u001b[38;5;28mbool\u001b[39m] = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    514\u001b[39m ) -> Tuple[torch.Tensor]:\n\u001b[32m    515\u001b[39m     self_outputs = \u001b[38;5;28mself\u001b[39m.self(\n\u001b[32m    516\u001b[39m         hidden_states,\n\u001b[32m    517\u001b[39m         attention_mask,\n\u001b[32m   (...)\u001b[39m\u001b[32m    522\u001b[39m         output_attentions,\n\u001b[32m    523\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m524\u001b[39m     attention_output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m(\u001b[49m\u001b[43mself_outputs\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    525\u001b[39m     outputs = (attention_output,) + self_outputs[\u001b[32m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n\u001b[32m    526\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.13/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.13/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.13/site-packages/transformers/models/bert/modeling_bert.py:466\u001b[39m, in \u001b[36mBertSelfOutput.forward\u001b[39m\u001b[34m(self, hidden_states, input_tensor)\u001b[39m\n\u001b[32m    465\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m466\u001b[39m     hidden_states = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdense\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    467\u001b[39m     hidden_states = \u001b[38;5;28mself\u001b[39m.dropout(hidden_states)\n\u001b[32m    468\u001b[39m     hidden_states = \u001b[38;5;28mself\u001b[39m.LayerNorm(hidden_states + input_tensor)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.13/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.13/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.13/site-packages/torch/nn/modules/linear.py:125\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import random\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "class SoftmaxTeacher(nn.Module):\n",
    "    def __init__(self, hidden_size=768):\n",
    "        super().__init__()\n",
    "        self.linear_q = nn.Linear(hidden_size, hidden_size)\n",
    "        self.linear_k = nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        Q = self.linear_q(x)  # (B, L, D)\n",
    "        K = self.linear_k(x)  # (B, L, D)\n",
    "        attn_logits = torch.matmul(Q, K.transpose(-1, -2)) / (x.size(-1) ** 0.5)  # (B, L, L)\n",
    "        attn_scores = torch.softmax(attn_logits, dim=-1)\n",
    "        return attn_scores\n",
    "\n",
    "class RandomTokenDataset(Dataset):\n",
    "    def __init__(self, tokenizer, num_samples=1000, seq_len=512):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.vocab = list(tokenizer.get_vocab().values())\n",
    "        self.num_samples = num_samples\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_ids = torch.tensor(\n",
    "            random.choices(self.vocab, k=self.seq_len), dtype=torch.long\n",
    "        )\n",
    "        attention_mask = torch.ones_like(input_ids)\n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "        }\n",
    "\n",
    "class BinaryTreeAttention(nn.Module):\n",
    "    def __init__(self, hidden_size=768, chunk_size=64):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.chunk_size = chunk_size\n",
    "        self.depth = int(math.log2(512 // chunk_size))\n",
    "\n",
    "        # self.convs = nn.ModuleList()\n",
    "        # max_chunks = 512 // self.chunk_size\n",
    "        # for i in range(self.depth):\n",
    "        #     k = min(2 ** (i + 1), max_chunks)\n",
    "        #     self.convs.append(\n",
    "        #         nn.Conv1d(\n",
    "        #             in_channels=hidden_size,\n",
    "        #             out_channels=hidden_size,\n",
    "        #             kernel_size=k,\n",
    "        #             stride=k,\n",
    "        #             padding=0\n",
    "        #         )\n",
    "        #     )\n",
    "        #     max_chunks = max_chunks // 2\n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Conv1d(\n",
    "                in_channels=hidden_size,\n",
    "                out_channels=hidden_size,\n",
    "                kernel_size=2 ** (i + 1),\n",
    "                stride=2 ** (i + 1),\n",
    "                padding=0\n",
    "            )\n",
    "            for i in range(self.depth)\n",
    "        ])\n",
    "\n",
    "\n",
    "        self.query_vectors = nn.ParameterList([\n",
    "            nn.Parameter(torch.randn(hidden_size)) for _ in range(self.depth)\n",
    "        ])\n",
    "\n",
    "        os.makedirs(\"visuals\", exist_ok=True)\n",
    "       \n",
    "    # def forward(self, x, query, label_positions=None):\n",
    "    #     B, L, D = x.shape\n",
    "    #     assert L % self.chunk_size == 0, \"Sequence length must be divisible by chunk size\"\n",
    "    #     num_chunks = L // self.chunk_size\n",
    "\n",
    "    #     x_chunks = x.view(B, num_chunks, self.chunk_size, D)\n",
    "    #     summaries = x_chunks.mean(dim=2)  # (B, num_chunks, D)\n",
    "    #     summaries = summaries.transpose(1, 2)  # (B, D, num_chunks)\n",
    "\n",
    "    #     tree_levels = [summaries.transpose(1, 2)]\n",
    "    #     for level in range(self.depth):\n",
    "    #         print(summaries.shape[-1], self.convs[level].kernel_size[0])\n",
    "    #         if summaries.shape[-1] < self.convs[level].kernel_size[0]:\n",
    "    #             break\n",
    "    #         # summaries = self.convs[level](summaries)  # (B, D, reduced_chunks)\n",
    "    #         tree_levels.append(self.convs[level](summaries).transpose(1, 2))  # (B, chunks, D)\n",
    "\n",
    "    #     for level in range(len(tree_levels) - 1):\n",
    "    #         q_proj = query[0] * self.query_vectors[level]  # visualize batch 0 only\n",
    "    #         q_proj = q_proj.detach().cpu().numpy()\n",
    "    #         plt.figure()\n",
    "    #         plt.plot(q_proj)\n",
    "    #         plt.title(f\"Query projection at level {level}\")\n",
    "    #         plt.xlabel(\"Dimension\")\n",
    "    #         plt.ylabel(\"Value\")\n",
    "    #         plt.savefig(f\"visuals/q_proj_level_{level}.png\")\n",
    "    #         plt.close()\n",
    "\n",
    "    #     attn_weights = torch.zeros(B, L, device=x.device)\n",
    "    #     per_level_loss = 0.0\n",
    "\n",
    "    #     for b in range(B):\n",
    "    #         q_i = query[b]  # (D,)\n",
    "    #         idx_range = list(range(num_chunks))\n",
    "    #         current_level = 0\n",
    "\n",
    "    #         if label_positions is not None:\n",
    "    #             target_chunk = label_positions[b].item() // self.chunk_size\n",
    "\n",
    "    #         while len(idx_range) > 1 and current_level < len(tree_levels) - 1:\n",
    "    #             next_range = []\n",
    "    #             losses = []\n",
    "    #             max_index = tree_levels[current_level].shape[1]\n",
    "    #             for j in range(0, len(idx_range), 2):\n",
    "    #                 left_idx = idx_range[j]\n",
    "    #                 if j + 1 >= len(idx_range):\n",
    "    #                     next_range.append(left_idx)\n",
    "    #                     continue\n",
    "    #                 right_idx = idx_range[j + 1]\n",
    "    #                 if left_idx >= max_index or right_idx >= max_index:\n",
    "    #                     continue\n",
    "    #                 left_sum = tree_levels[current_level][b, left_idx]\n",
    "    #                 right_sum = tree_levels[current_level][b, right_idx]\n",
    "    #                 q_proj = q_i * self.query_vectors[current_level]\n",
    "    #                 score_left = torch.dot(q_proj, left_sum)\n",
    "    #                 score_right = torch.dot(q_proj, right_sum)\n",
    "    #                 logit = torch.stack([score_left, score_right])\n",
    "\n",
    "    #                 if label_positions is not None:\n",
    "    #                     decision = 0 if target_chunk % 2 == 0 else 1\n",
    "    #                     loss = F.cross_entropy(logit.unsqueeze(0), torch.tensor([decision], device=x.device))\n",
    "    #                     target_chunk = target_chunk // 2\n",
    "    #                 else:\n",
    "    #                     loss = torch.tensor(0.0, device=x.device)\n",
    "    #                     decision = torch.argmax(logit).item()\n",
    "\n",
    "    #                 losses.append(loss)\n",
    "    #                 chosen = left_idx if decision == 0 else right_idx\n",
    "    #                 next_range.append(chosen)\n",
    "\n",
    "    #             idx_range = next_range\n",
    "    #             current_level += 1\n",
    "    #             if losses:\n",
    "    #                 per_level_loss += sum(losses) / len(losses)\n",
    "\n",
    "    #         final_chunk = idx_range[0]\n",
    "    #         start = final_chunk * self.chunk_size\n",
    "    #         end = start + self.chunk_size\n",
    "    #         local_scores = torch.matmul(q_i, x[b, start:end].T)\n",
    "    #         local_attn = F.softmax(local_scores, dim=-1)\n",
    "    #         attn_weights[b, start:end] = local_attn\n",
    "\n",
    "    #     return attn_weights, per_level_loss / B\n",
    "\n",
    "    def build_summaries(self, x):\n",
    "        B, L, D = x.shape\n",
    "        assert L % self.chunk_size == 0, \"Sequence length must be divisible by chunk size\"\n",
    "        num_chunks = L // self.chunk_size\n",
    "\n",
    "        x_chunks = x.view(B, num_chunks, self.chunk_size, D)\n",
    "        leaf_summaries = x_chunks.mean(dim=2)  # (B, num_chunks, D)\n",
    "\n",
    "        all_levels = [leaf_summaries]\n",
    "        # for conv in self.convs:\n",
    "        #     input_summary = leaf_summaries.transpose(1, 2)  # (B, D, num_chunks)\n",
    "        #     print(input_summary.size(-1), conv.kernel_size[0])\n",
    "        #     if input_summary.size(-1) < conv.kernel_size[0]:\n",
    "        #         break\n",
    "        #     merged = conv(input_summary)  # (B, D, reduced_chunks)\n",
    "        #     all_levels.append(merged.transpose(1, 2))  # back to (B, chunks, D)\n",
    "        for conv in self.convs:\n",
    "            if leaf_summaries.size(1) < conv.kernel_size[0]:\n",
    "                break\n",
    "            input_summary = leaf_summaries.transpose(1, 2)  # (B, D, chunks)\n",
    "            merged = conv(input_summary)  # (B, D, reduced_chunks)\n",
    "            all_levels.append(merged.transpose(1, 2))  # (B, reduced_chunks, D)\n",
    "\n",
    "        # for i, level in enumerate(all_levels):\n",
    "        #     print(f\"Level {i}: {level.shape}\")\n",
    "            \n",
    "        return all_levels\n",
    "\n",
    "    def forward(self, x, query, label_positions=None):\n",
    "        B, L, D = x.shape\n",
    "        tree_levels = self.build_summaries(x)  # List of (B, chunks, D)\n",
    "        attn_weights = torch.zeros(B, L, device=x.device)\n",
    "        per_level_loss = 0.0\n",
    "\n",
    "        for level in range(self.depth):\n",
    "            q_proj = query[0] * self.query_vectors[level]\n",
    "            q_proj = q_proj.detach().cpu().numpy()\n",
    "            plt.figure()\n",
    "            plt.plot(q_proj)\n",
    "            plt.title(f\"Query projection at level {level}\")\n",
    "            plt.xlabel(\"Dimension\")\n",
    "            plt.ylabel(\"Value\")\n",
    "            plt.savefig(f\"visuals/q_proj_level_{level}.png\")\n",
    "            plt.close()\n",
    "\n",
    "        for b in range(B):\n",
    "            q_i = query[b]  # (D,)\n",
    "\n",
    "            if label_positions is not None:\n",
    "                target_token = label_positions[b].item()\n",
    "                # print(target_token)\n",
    "                target_chunk = target_token // self.chunk_size\n",
    "                # print(f'{target_chunk}/{512 // self.chunk_size}')\n",
    "                path = []\n",
    "                node = target_chunk\n",
    "                for level in reversed(range(self.depth)):\n",
    "                    path.append(node)\n",
    "                    node = node // 2\n",
    "                path = list(reversed(path))\n",
    "\n",
    "                for level, chunk_idx in enumerate(path):\n",
    "                    # if level >= len(tree_levels):\n",
    "                    #     break\n",
    "\n",
    "                    summaries = tree_levels[self.depth - level - 1][b]  # (chunks, D)\n",
    "                    # if chunk_idx * 2 + 1 >= summaries.size(0):\n",
    "                    #     continue\n",
    "                    # print(len(summaries), chunk_idx)\n",
    "                    left = summaries[chunk_idx // 2 * 2]\n",
    "                    right = summaries[chunk_idx // 2 * 2 + 1]\n",
    "\n",
    "                    q_proj = q_i * self.query_vectors[level]\n",
    "                    score_left = torch.dot(q_proj, left)\n",
    "                    score_right = torch.dot(q_proj, right)\n",
    "                    logit = torch.stack([score_left, score_right])\n",
    "                    decision = 0 if (chunk_idx % 2 == 0) else 1\n",
    "                    loss = F.cross_entropy(logit.unsqueeze(0), torch.tensor([decision], device=x.device))\n",
    "                    per_level_loss += loss\n",
    "\n",
    "                final_chunk = path[-1]\n",
    "                start = final_chunk * self.chunk_size\n",
    "                end = start + self.chunk_size\n",
    "                if end <= L:\n",
    "                    local_scores = torch.matmul(q_i, x[b, start:end].T)\n",
    "                    local_attn = F.softmax(local_scores, dim=-1)\n",
    "                    attn_weights[b, start:end] = local_attn\n",
    "\n",
    "        return attn_weights, per_level_loss / B\n",
    "\n",
    "\n",
    "class TreeAttentionModel(nn.Module):\n",
    "    def __init__(self, encoder, tree_model):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.tree_model = tree_model\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, label_pos, teacher_attn):\n",
    "        with torch.no_grad():\n",
    "            x = self.encoder(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state\n",
    "        query = x[:, 0, :]  # (B, D), assuming CLS token is used as the query\n",
    "        tree_attn, gate_loss = self.tree_model(x, query, label_pos)\n",
    "        attn_loss = F.kl_div((tree_attn + 1e-8).log(), teacher_attn[:, 0], reduction='batchmean')\n",
    "        return attn_loss + gate_loss\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "    bert = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "    teacher = SoftmaxTeacher(hidden_size=768)\n",
    "    dataset = RandomTokenDataset(tokenizer=tokenizer, num_samples=1000, seq_len=512)\n",
    "    dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
    "\n",
    "    tree_model = BinaryTreeAttention(hidden_size=768, chunk_size=64)\n",
    "    model = TreeAttentionModel(bert, tree_model)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "    for epoch in range(3):\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch[\"input_ids\"]\n",
    "            attention_mask = batch[\"attention_mask\"]\n",
    "\n",
    "            with torch.no_grad():\n",
    "                x = bert(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state\n",
    "                teacher_attn = teacher(x)\n",
    "                topk = torch.topk(teacher_attn[:, 0], k=1, dim=-1).indices.squeeze(-1)\n",
    "\n",
    "            loss = model(input_ids, attention_mask, topk, teacher_attn)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            print(\"Loss:\", loss.item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary Mergers Tree Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 13.965141296386719\n",
      "Loss: 13.2096586227417\n",
      "Loss: 13.895458221435547\n",
      "Loss: 17.250762939453125\n",
      "Loss: 13.641131401062012\n",
      "Loss: 13.940834045410156\n",
      "Loss: 15.438117980957031\n",
      "Loss: 16.266212463378906\n",
      "Loss: 13.824352264404297\n",
      "Loss: 14.223379135131836\n",
      "Loss: 15.071582794189453\n",
      "Loss: 17.11172103881836\n",
      "Loss: 14.512250900268555\n",
      "Loss: 16.729019165039062\n",
      "Loss: 16.687368392944336\n",
      "Loss: 15.245388984680176\n",
      "Loss: 13.9462308883667\n",
      "Loss: 13.948664665222168\n",
      "Loss: 18.147167205810547\n",
      "Loss: 13.607330322265625\n",
      "Loss: 15.21592903137207\n",
      "Loss: 16.46346092224121\n",
      "Loss: 13.606891632080078\n",
      "Loss: 15.262789726257324\n",
      "Loss: 17.201038360595703\n",
      "Loss: 15.359214782714844\n",
      "Loss: 14.675223350524902\n",
      "Loss: 13.768728256225586\n",
      "Loss: 14.734827995300293\n",
      "Loss: 13.728902816772461\n",
      "Loss: 14.884159088134766\n",
      "Loss: 20.467571258544922\n",
      "Loss: 12.927183151245117\n",
      "Loss: 13.422428131103516\n",
      "Loss: 13.62571907043457\n",
      "Loss: 14.110284805297852\n",
      "Loss: 14.000645637512207\n",
      "Loss: 14.118144989013672\n",
      "Loss: 13.953058242797852\n",
      "Loss: 20.6766414642334\n",
      "Loss: 13.024291038513184\n",
      "Loss: 15.373546600341797\n",
      "Loss: 12.129661560058594\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 166\u001b[39m\n\u001b[32m    163\u001b[39m attention_mask = batch[\u001b[33m\"\u001b[39m\u001b[33mattention_mask\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    165\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m--> \u001b[39m\u001b[32m166\u001b[39m     x = \u001b[43mbert\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m)\u001b[49m.last_hidden_state\n\u001b[32m    167\u001b[39m     teacher_attn = teacher(x)\n\u001b[32m    168\u001b[39m     topk = torch.topk(teacher_attn[:, \u001b[32m0\u001b[39m], k=\u001b[32m1\u001b[39m, dim=-\u001b[32m1\u001b[39m).indices.squeeze(-\u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.13/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.13/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.13/site-packages/transformers/models/bert/modeling_bert.py:1142\u001b[39m, in \u001b[36mBertModel.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m   1135\u001b[39m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[32m   1136\u001b[39m \u001b[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[32m   1137\u001b[39m \u001b[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[32m   1138\u001b[39m \u001b[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[32m   1139\u001b[39m \u001b[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[32m   1140\u001b[39m head_mask = \u001b[38;5;28mself\u001b[39m.get_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m.config.num_hidden_layers)\n\u001b[32m-> \u001b[39m\u001b[32m1142\u001b[39m encoder_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1143\u001b[39m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1144\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1145\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1146\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1147\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1148\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1149\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1150\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1151\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1152\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1153\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1154\u001b[39m sequence_output = encoder_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m   1155\u001b[39m pooled_output = \u001b[38;5;28mself\u001b[39m.pooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.pooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.13/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.13/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.13/site-packages/transformers/models/bert/modeling_bert.py:695\u001b[39m, in \u001b[36mBertEncoder.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m    684\u001b[39m     layer_outputs = \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(\n\u001b[32m    685\u001b[39m         layer_module.\u001b[34m__call__\u001b[39m,\n\u001b[32m    686\u001b[39m         hidden_states,\n\u001b[32m   (...)\u001b[39m\u001b[32m    692\u001b[39m         output_attentions,\n\u001b[32m    693\u001b[39m     )\n\u001b[32m    694\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m695\u001b[39m     layer_outputs = \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    696\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    697\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    698\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    699\u001b[39m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    700\u001b[39m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    701\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    702\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    703\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    705\u001b[39m hidden_states = layer_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    706\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.13/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.13/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.13/site-packages/transformers/models/bert/modeling_bert.py:627\u001b[39m, in \u001b[36mBertLayer.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[39m\n\u001b[32m    624\u001b[39m     cross_attn_present_key_value = cross_attention_outputs[-\u001b[32m1\u001b[39m]\n\u001b[32m    625\u001b[39m     present_key_value = present_key_value + cross_attn_present_key_value\n\u001b[32m--> \u001b[39m\u001b[32m627\u001b[39m layer_output = \u001b[43mapply_chunking_to_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    628\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfeed_forward_chunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mchunk_size_feed_forward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mseq_len_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_output\u001b[49m\n\u001b[32m    629\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    630\u001b[39m outputs = (layer_output,) + outputs\n\u001b[32m    632\u001b[39m \u001b[38;5;66;03m# if decoder, return the attn key/values as the last output\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.13/site-packages/transformers/pytorch_utils.py:258\u001b[39m, in \u001b[36mapply_chunking_to_forward\u001b[39m\u001b[34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[39m\n\u001b[32m    255\u001b[39m     \u001b[38;5;66;03m# concatenate output at same dimension\u001b[39;00m\n\u001b[32m    256\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m torch.cat(output_chunks, dim=chunk_dim)\n\u001b[32m--> \u001b[39m\u001b[32m258\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43minput_tensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.13/site-packages/transformers/models/bert/modeling_bert.py:639\u001b[39m, in \u001b[36mBertLayer.feed_forward_chunk\u001b[39m\u001b[34m(self, attention_output)\u001b[39m\n\u001b[32m    638\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34mfeed_forward_chunk\u001b[39m(\u001b[38;5;28mself\u001b[39m, attention_output):\n\u001b[32m--> \u001b[39m\u001b[32m639\u001b[39m     intermediate_output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mintermediate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattention_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    640\u001b[39m     layer_output = \u001b[38;5;28mself\u001b[39m.output(intermediate_output, attention_output)\n\u001b[32m    641\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m layer_output\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.13/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.13/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.13/site-packages/transformers/models/bert/modeling_bert.py:539\u001b[39m, in \u001b[36mBertIntermediate.forward\u001b[39m\u001b[34m(self, hidden_states)\u001b[39m\n\u001b[32m    538\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states: torch.Tensor) -> torch.Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m539\u001b[39m     hidden_states = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdense\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    540\u001b[39m     hidden_states = \u001b[38;5;28mself\u001b[39m.intermediate_act_fn(hidden_states)\n\u001b[32m    541\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.13/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.13/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.13/site-packages/torch/nn/modules/linear.py:125\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import random\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "class SoftmaxTeacher(nn.Module):\n",
    "    def __init__(self, hidden_size=768):\n",
    "        super().__init__()\n",
    "        self.linear_q = nn.Linear(hidden_size, hidden_size)\n",
    "        self.linear_k = nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        Q = self.linear_q(x)\n",
    "        K = self.linear_k(x)\n",
    "        attn_logits = torch.matmul(Q, K.transpose(-1, -2)) / (x.size(-1) ** 0.5)\n",
    "        attn_scores = torch.softmax(attn_logits, dim=-1)\n",
    "        return attn_scores\n",
    "\n",
    "class RandomTokenDataset(Dataset):\n",
    "    def __init__(self, tokenizer, num_samples=1000, seq_len=512):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.vocab = list(tokenizer.get_vocab().values())\n",
    "        self.num_samples = num_samples\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_ids = torch.tensor(\n",
    "            random.choices(self.vocab, k=self.seq_len), dtype=torch.long\n",
    "        )\n",
    "        attention_mask = torch.ones_like(input_ids)\n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "        }\n",
    "\n",
    "class BinaryTreeAttention(nn.Module):\n",
    "    def __init__(self, hidden_size=768, chunk_size=64):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.chunk_size = chunk_size\n",
    "        self.depth = int(math.log2(512 // chunk_size))\n",
    "\n",
    "        self.merge_layers = nn.ModuleList([\n",
    "            nn.Linear(2 * hidden_size, hidden_size) for _ in range(self.depth)\n",
    "        ])\n",
    "\n",
    "        self.query_vectors = nn.ParameterList([\n",
    "            nn.Parameter(torch.randn(hidden_size)) for _ in range(self.depth)\n",
    "        ])\n",
    "\n",
    "        os.makedirs(\"visuals\", exist_ok=True)\n",
    "\n",
    "    def forward(self, x, query, label_positions=None):\n",
    "        B, L, D = x.shape\n",
    "        assert L % self.chunk_size == 0, \"Sequence length must be divisible by chunk size\"\n",
    "        num_chunks = L // self.chunk_size\n",
    "\n",
    "        x_chunks = x.view(B, num_chunks, self.chunk_size, D)\n",
    "        summaries = x_chunks.mean(dim=2)  # (B, num_chunks, D)\n",
    "\n",
    "        attn_weights = torch.zeros(B, L, device=x.device)\n",
    "        per_level_loss = 0.0\n",
    "\n",
    "        for level in range(self.depth):\n",
    "            q_proj = query[0] * self.query_vectors[level]\n",
    "            q_proj = q_proj.detach().cpu().numpy()\n",
    "            plt.figure()\n",
    "            plt.plot(q_proj)\n",
    "            plt.title(f\"Query projection at level {level}\")\n",
    "            plt.xlabel(\"Dimension\")\n",
    "            plt.ylabel(\"Value\")\n",
    "            plt.savefig(f\"visuals/q_proj_level_{level}.png\")\n",
    "            plt.close()\n",
    "\n",
    "        for b in range(B):\n",
    "            q_i = query[b]  # (D,)\n",
    "            idx_range = list(range(num_chunks))\n",
    "            current_level = 0\n",
    "\n",
    "            if label_positions is not None:\n",
    "                target_chunk = label_positions[b].item() // self.chunk_size\n",
    "\n",
    "            while len(idx_range) > 1 and current_level < self.depth:\n",
    "                next_range = []\n",
    "                losses = []\n",
    "                for j in range(0, len(idx_range), 2):\n",
    "                    left_idx = idx_range[j]\n",
    "                    if j + 1 >= len(idx_range):\n",
    "                        next_range.append(left_idx)\n",
    "                        continue\n",
    "                    right_idx = idx_range[j + 1]\n",
    "                    left = summaries[b, left_idx]\n",
    "                    right = summaries[b, right_idx]\n",
    "                    merged = self.merge_layers[current_level](torch.cat([left, right], dim=-1))\n",
    "                    q_proj = q_i * self.query_vectors[current_level]\n",
    "                    score_left = torch.dot(q_proj, left)\n",
    "                    score_right = torch.dot(q_proj, right)\n",
    "                    logit = torch.stack([score_left, score_right])\n",
    "\n",
    "                    if label_positions is not None:\n",
    "                        decision = 0 if target_chunk % 2 == 0 else 1\n",
    "                        loss = F.cross_entropy(logit.unsqueeze(0), torch.tensor([decision], device=x.device))\n",
    "                        target_chunk = target_chunk // 2\n",
    "                    else:\n",
    "                        loss = torch.tensor(0.0, device=x.device)\n",
    "                        decision = torch.argmax(logit).item()\n",
    "\n",
    "                    losses.append(loss)\n",
    "                    chosen = left_idx if decision == 0 else right_idx\n",
    "                    next_range.append(chosen)\n",
    "                idx_range = next_range\n",
    "                current_level += 1\n",
    "                if losses:\n",
    "                    per_level_loss += sum(losses) / len(losses)\n",
    "\n",
    "            final_chunk = idx_range[0]\n",
    "            start = final_chunk * self.chunk_size\n",
    "            end = start + self.chunk_size\n",
    "            local_scores = torch.matmul(q_i, x[b, start:end].T)\n",
    "            local_attn = F.softmax(local_scores, dim=-1)\n",
    "            attn_weights[b, start:end] = local_attn\n",
    "\n",
    "        return attn_weights, per_level_loss / B\n",
    "\n",
    "class TreeAttentionModel(nn.Module):\n",
    "    def __init__(self, encoder, tree_model):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.tree_model = tree_model\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, label_pos, teacher_attn):\n",
    "        with torch.no_grad():\n",
    "            x = self.encoder(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state\n",
    "        query = x[:, 0, :]\n",
    "        tree_attn, gate_loss = self.tree_model(x, query, label_pos)\n",
    "        attn_loss = F.kl_div((tree_attn + 1e-8).log(), teacher_attn[:, 0], reduction='batchmean')\n",
    "        return attn_loss + gate_loss\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "    bert = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "    teacher = SoftmaxTeacher(hidden_size=768)\n",
    "    dataset = RandomTokenDataset(tokenizer=tokenizer, num_samples=1000, seq_len=512)\n",
    "    dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
    "\n",
    "    tree_model = BinaryTreeAttention(hidden_size=768, chunk_size=64)\n",
    "    model = TreeAttentionModel(bert, tree_model)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "    for epoch in range(3):\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch[\"input_ids\"]\n",
    "            attention_mask = batch[\"attention_mask\"]\n",
    "\n",
    "            with torch.no_grad():\n",
    "                x = bert(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state\n",
    "                teacher_attn = teacher(x)\n",
    "                topk = torch.topk(teacher_attn[:, 0], k=1, dim=-1).indices.squeeze(-1)\n",
    "\n",
    "            loss = model(input_ids, attention_mask, topk, teacher_attn)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            print(\"Loss:\", loss.item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self Attention Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 23.362707138061523\n",
      "Loss: 24.99822998046875\n",
      "Loss: 21.739498138427734\n",
      "Loss: 21.880748748779297\n",
      "Loss: 21.961990356445312\n",
      "Loss: 24.311073303222656\n",
      "Loss: 20.525001525878906\n",
      "Loss: 20.038055419921875\n",
      "Loss: 19.34503936767578\n",
      "Loss: 16.988056182861328\n",
      "Loss: 20.15441131591797\n",
      "Loss: 23.3741397857666\n",
      "Loss: 19.297767639160156\n",
      "Loss: 22.010738372802734\n",
      "Loss: 24.37759780883789\n",
      "Loss: 23.678829193115234\n",
      "Loss: 20.929767608642578\n",
      "Loss: 20.05202865600586\n",
      "Loss: 21.520214080810547\n",
      "Loss: 17.408260345458984\n",
      "Loss: 20.376911163330078\n",
      "Loss: 23.20523452758789\n",
      "Loss: 21.76390838623047\n",
      "Loss: 19.359477996826172\n",
      "Loss: 23.303855895996094\n",
      "Loss: 19.793548583984375\n",
      "Loss: 18.40351104736328\n",
      "Loss: 19.889476776123047\n",
      "Loss: 20.13365936279297\n",
      "Loss: 19.169321060180664\n",
      "Loss: 17.88991355895996\n",
      "Loss: 18.278301239013672\n",
      "Loss: 21.093997955322266\n",
      "Loss: 21.939958572387695\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[54]\u001b[39m\u001b[32m, line 221\u001b[39m\n\u001b[32m    218\u001b[39m attention_mask = batch[\u001b[33m\"\u001b[39m\u001b[33mattention_mask\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    220\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m--> \u001b[39m\u001b[32m221\u001b[39m     x = \u001b[43mbert\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m)\u001b[49m.last_hidden_state\n\u001b[32m    222\u001b[39m     teacher_attn = teacher(x)\n\u001b[32m    223\u001b[39m     topk = torch.topk(teacher_attn[:, \u001b[32m0\u001b[39m], k=\u001b[32m1\u001b[39m, dim=-\u001b[32m1\u001b[39m).indices.squeeze(-\u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.13/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.13/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.13/site-packages/transformers/models/bert/modeling_bert.py:1142\u001b[39m, in \u001b[36mBertModel.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m   1135\u001b[39m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[32m   1136\u001b[39m \u001b[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[32m   1137\u001b[39m \u001b[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[32m   1138\u001b[39m \u001b[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[32m   1139\u001b[39m \u001b[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[32m   1140\u001b[39m head_mask = \u001b[38;5;28mself\u001b[39m.get_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m.config.num_hidden_layers)\n\u001b[32m-> \u001b[39m\u001b[32m1142\u001b[39m encoder_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1143\u001b[39m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1144\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1145\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1146\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1147\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1148\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1149\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1150\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1151\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1152\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1153\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1154\u001b[39m sequence_output = encoder_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m   1155\u001b[39m pooled_output = \u001b[38;5;28mself\u001b[39m.pooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.pooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.13/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.13/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.13/site-packages/transformers/models/bert/modeling_bert.py:695\u001b[39m, in \u001b[36mBertEncoder.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m    684\u001b[39m     layer_outputs = \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(\n\u001b[32m    685\u001b[39m         layer_module.\u001b[34m__call__\u001b[39m,\n\u001b[32m    686\u001b[39m         hidden_states,\n\u001b[32m   (...)\u001b[39m\u001b[32m    692\u001b[39m         output_attentions,\n\u001b[32m    693\u001b[39m     )\n\u001b[32m    694\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m695\u001b[39m     layer_outputs = \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    696\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    697\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    698\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    699\u001b[39m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    700\u001b[39m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    701\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    702\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    703\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    705\u001b[39m hidden_states = layer_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    706\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.13/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.13/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.13/site-packages/transformers/models/bert/modeling_bert.py:627\u001b[39m, in \u001b[36mBertLayer.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[39m\n\u001b[32m    624\u001b[39m     cross_attn_present_key_value = cross_attention_outputs[-\u001b[32m1\u001b[39m]\n\u001b[32m    625\u001b[39m     present_key_value = present_key_value + cross_attn_present_key_value\n\u001b[32m--> \u001b[39m\u001b[32m627\u001b[39m layer_output = \u001b[43mapply_chunking_to_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    628\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfeed_forward_chunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mchunk_size_feed_forward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mseq_len_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_output\u001b[49m\n\u001b[32m    629\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    630\u001b[39m outputs = (layer_output,) + outputs\n\u001b[32m    632\u001b[39m \u001b[38;5;66;03m# if decoder, return the attn key/values as the last output\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.13/site-packages/transformers/pytorch_utils.py:258\u001b[39m, in \u001b[36mapply_chunking_to_forward\u001b[39m\u001b[34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[39m\n\u001b[32m    255\u001b[39m     \u001b[38;5;66;03m# concatenate output at same dimension\u001b[39;00m\n\u001b[32m    256\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m torch.cat(output_chunks, dim=chunk_dim)\n\u001b[32m--> \u001b[39m\u001b[32m258\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43minput_tensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.13/site-packages/transformers/models/bert/modeling_bert.py:640\u001b[39m, in \u001b[36mBertLayer.feed_forward_chunk\u001b[39m\u001b[34m(self, attention_output)\u001b[39m\n\u001b[32m    638\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34mfeed_forward_chunk\u001b[39m(\u001b[38;5;28mself\u001b[39m, attention_output):\n\u001b[32m    639\u001b[39m     intermediate_output = \u001b[38;5;28mself\u001b[39m.intermediate(attention_output)\n\u001b[32m--> \u001b[39m\u001b[32m640\u001b[39m     layer_output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m(\u001b[49m\u001b[43mintermediate_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    641\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m layer_output\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.13/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.13/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.13/site-packages/transformers/models/bert/modeling_bert.py:552\u001b[39m, in \u001b[36mBertOutput.forward\u001b[39m\u001b[34m(self, hidden_states, input_tensor)\u001b[39m\n\u001b[32m    551\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m552\u001b[39m     hidden_states = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdense\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    553\u001b[39m     hidden_states = \u001b[38;5;28mself\u001b[39m.dropout(hidden_states)\n\u001b[32m    554\u001b[39m     hidden_states = \u001b[38;5;28mself\u001b[39m.LayerNorm(hidden_states + input_tensor)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.13/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.13/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.13/site-packages/torch/nn/modules/linear.py:125\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "class BinaryTreeAttention(nn.Module):\n",
    "    def __init__(self, hidden_size=768, chunk_size=64, depth=5):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.chunk_size = chunk_size\n",
    "        self.depth = depth\n",
    "\n",
    "        self.query_vectors = nn.ParameterList([\n",
    "            nn.Parameter(torch.randn(hidden_size)) for _ in range(depth)\n",
    "        ])\n",
    "\n",
    "        self.router_vectors = nn.ParameterList([\n",
    "            nn.Parameter(torch.randn(hidden_size)) for _ in range(depth)\n",
    "        ])\n",
    "\n",
    "        os.makedirs(\"visuals\", exist_ok=True)\n",
    "        \n",
    "    def build_summaries(self, x):\n",
    "        B, L, D = x.shape\n",
    "        assert L % self.chunk_size == 0, \"Sequence length must be divisible by chunk size\"\n",
    "        num_chunks = L // self.chunk_size\n",
    "\n",
    "        x_chunks = x.view(B, num_chunks, self.chunk_size, D)\n",
    "        leaf_summaries = x_chunks.mean(dim=2)  # (B, num_chunks, D)\n",
    "\n",
    "        all_levels = [leaf_summaries]\n",
    "\n",
    "        for level, router in enumerate(self.router_vectors):\n",
    "            group_size = 2 ** level\n",
    "            num_groups = num_chunks // group_size\n",
    "            summaries = []\n",
    "\n",
    "            if num_groups == 0:\n",
    "                break\n",
    "\n",
    "            for g in range(num_groups):\n",
    "                group = leaf_summaries[:, g * group_size:(g + 1) * group_size, :]  # (B, group_size, D)\n",
    "                logits = torch.einsum('bnd,d->bn', group, router)  # (B, group_size)\n",
    "                attn_weights = torch.softmax(logits, dim=-1).unsqueeze(-1)  # (B, group_size, 1)\n",
    "                summary = (attn_weights * group).sum(dim=1)  # (B, D)\n",
    "                summaries.append(summary.unsqueeze(1))\n",
    "\n",
    "            summaries = torch.cat(summaries, dim=1)  # (B, num_groups, D)\n",
    "            all_levels.append(summaries)\n",
    "\n",
    "        return all_levels\n",
    "\n",
    "    def forward(self, x, query, label_positions=None):\n",
    "        B, L, D = x.shape\n",
    "        tree_levels = self.build_summaries(x)  # List of (B, chunks, D)\n",
    "        attn_weights = torch.zeros(B, L, device=x.device)\n",
    "        per_level_loss = 0.0\n",
    "\n",
    "        for level in range(self.depth):\n",
    "            q_proj = query[0] * self.query_vectors[level]\n",
    "            q_plot = q_proj.detach().cpu().numpy()\n",
    "            plt.figure()\n",
    "            plt.plot(q_plot)\n",
    "            plt.title(f\"Query projection at level {level}\")\n",
    "            plt.xlabel(\"Dimension\")\n",
    "            plt.ylabel(\"Value\")\n",
    "            plt.savefig(f\"visuals/q_proj_level_{level}.png\")\n",
    "            plt.close()\n",
    "\n",
    "        for b in range(B):\n",
    "            q_i = query[b]  # (D,)\n",
    "\n",
    "            if label_positions is not None:\n",
    "                target_token = label_positions[b].item()\n",
    "                target_chunk = target_token // self.chunk_size\n",
    "                path = []\n",
    "                node = target_chunk\n",
    "                for level in reversed(range(self.depth)):\n",
    "                    path.append(node)\n",
    "                    node = node // 2\n",
    "                path = list(reversed(path))\n",
    "\n",
    "                for level, chunk_idx in enumerate(path):\n",
    "                    summaries = tree_levels[self.depth - level - 2][b]  # (num_summaries, D)\n",
    "\n",
    "                    q_proj = q_i * self.query_vectors[level]  # (D,)\n",
    "                    logits = torch.matmul(summaries, q_proj)  # (num_summaries,)\n",
    "                    weights = F.softmax(logits, dim=0)  # (num_summaries,)\n",
    "\n",
    "                    # Find the correct index for supervision\n",
    "                    target_idx = chunk_idx // (2 ** level)\n",
    "                    target_weight = weights[target_idx]\n",
    "                    loss = -torch.log(target_weight + 1e-8)\n",
    "                    per_level_loss += loss\n",
    "\n",
    "                final_chunk = path[-1]\n",
    "                start = final_chunk * self.chunk_size\n",
    "                end = start + self.chunk_size\n",
    "                if end <= L:\n",
    "                    local_scores = torch.matmul(q_i, x[b, start:end].T)\n",
    "                    local_attn = F.softmax(local_scores, dim=-1)\n",
    "                    attn_weights[b, start:end] = local_attn\n",
    "\n",
    "        return attn_weights, per_level_loss / B\n",
    "\n",
    "    # def build_summaries(self, x):\n",
    "    #     B, L, D = x.shape\n",
    "    #     assert L % self.chunk_size == 0, \"Sequence length must be divisible by chunk size\"\n",
    "    #     num_chunks = L // self.chunk_size\n",
    "\n",
    "    #     x_chunks = x.view(B, num_chunks, self.chunk_size, D)\n",
    "    #     leaf_summaries = x_chunks.mean(dim=2)  # (B, num_chunks, D)\n",
    "\n",
    "    #     all_levels = [leaf_summaries]\n",
    "\n",
    "    #     for level, router in enumerate(self.router_vectors):\n",
    "    #         group_size = 2 ** level\n",
    "    #         num_groups = num_chunks // group_size\n",
    "            \n",
    "    #         if num_groups == 1:\n",
    "    #             break\n",
    "    #         summaries = []\n",
    "\n",
    "    #         for g in range(num_groups):\n",
    "    #             group = leaf_summaries[:, g * group_size:(g + 1) * group_size, :]  # (B, group_size, D)\n",
    "    #             logits = torch.einsum('bnd,d->bn', group, router)  # (B, group_size)\n",
    "    #             attn_weights = torch.softmax(logits, dim=-1).unsqueeze(-1)  # (B, group_size, 1)\n",
    "    #             summary = (attn_weights * group).sum(dim=1)  # (B, D)\n",
    "    #             summaries.append(summary.unsqueeze(1))\n",
    "\n",
    "    #         summaries = torch.cat(summaries, dim=1)  # (B, num_groups, D)\n",
    "    #         all_levels.append(summaries)\n",
    "\n",
    "    #     return all_levels\n",
    "\n",
    "    # def forward(self, x, query, label_positions=None):\n",
    "    #     B, L, D = x.shape\n",
    "    #     tree_levels = self.build_summaries(x)  # List of (B, chunks, D)\n",
    "    #     attn_weights = torch.zeros(B, L, device=x.device)\n",
    "    #     per_level_loss = 0.0\n",
    "\n",
    "    #     for level in range(self.depth):\n",
    "    #         q_proj = query[0] * self.query_vectors[level]\n",
    "    #         q_plot = q_proj.detach().cpu().numpy()\n",
    "    #         plt.figure()\n",
    "    #         plt.plot(q_plot)\n",
    "    #         plt.title(f\"Query projection at level {level}\")\n",
    "    #         plt.xlabel(\"Dimension\")\n",
    "    #         plt.ylabel(\"Value\")\n",
    "    #         plt.savefig(f\"visuals/q_proj_level_{level}.png\")\n",
    "    #         plt.close()\n",
    "\n",
    "    #     for b in range(B):\n",
    "    #         q_i = query[b]  # (D,)\n",
    "\n",
    "    #         if label_positions is not None:\n",
    "    #             target_token = label_positions[b].item()\n",
    "    #             # print(target_token)\n",
    "    #             target_chunk = target_token // self.chunk_size\n",
    "    #             path = []\n",
    "    #             node = target_chunk\n",
    "    #             # print(f'{target_chunk}/{512 // self.chunk_size}')\n",
    "    #             for level in reversed(range(self.depth - 1)):\n",
    "    #                 path.append(node)\n",
    "    #                 node = node // 2\n",
    "    #             path = list(reversed(path))\n",
    "\n",
    "    #             for level, chunk_idx in enumerate(path):\n",
    "    #                 # if self.depth - level - 1 >= len(tree_levels):\n",
    "    #                 #     break\n",
    "    #                 # print(len(tree_levels), self.depth)\n",
    "    #                 summaries = tree_levels[self.depth - level - 2][b]  # (chunks, D)\n",
    "    #                 # if chunk_idx * 2 + 1 >= summaries.size(0):\n",
    "    #                 #     continue\n",
    "    #                 # print(len(summaries), chunk_idx)\n",
    "    #                 left = summaries[chunk_idx // 2 * 2]\n",
    "    #                 right = summaries[chunk_idx // 2 * 2 + 1]\n",
    "\n",
    "    #                 q_proj = q_i * self.query_vectors[level]\n",
    "    #                 score_left = torch.dot(q_proj, left)\n",
    "    #                 score_right = torch.dot(q_proj, right)\n",
    "    #                 logit = torch.stack([score_left, score_right])\n",
    "    #                 decision = 0 if (chunk_idx % 2 == 0) else 1\n",
    "    #                 label = torch.tensor([decision], dtype=torch.long, device=x.device)\n",
    "    #                 loss = F.cross_entropy(logit.unsqueeze(0), label)\n",
    "\n",
    "    #                 # loss = F.cross_entropy(logit.unsqueeze(0), torch.tensor([decision], device=x.device))\n",
    "    #                 per_level_loss += loss\n",
    "    #                 # print(per_level_loss.requires_grad)\n",
    "\n",
    "    #             final_chunk = path[-1]\n",
    "    #             start = final_chunk * self.chunk_size\n",
    "    #             end = start + self.chunk_size\n",
    "    #             if end <= L:\n",
    "    #                 local_scores = torch.matmul(q_i, x[b, start:end].T)\n",
    "    #                 local_attn = F.softmax(local_scores, dim=-1)\n",
    "    #                 attn_weights[b, start:end] = local_attn\n",
    "\n",
    "    #     return attn_weights, per_level_loss / B\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "    bert = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "    teacher = SoftmaxTeacher(hidden_size=768)\n",
    "    dataset = RandomTokenDataset(tokenizer=tokenizer, num_samples=1000, seq_len=512)\n",
    "    dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
    "\n",
    "    tree_model = BinaryTreeAttention(hidden_size=768, chunk_size=64)\n",
    "    model = TreeAttentionModel(bert, tree_model)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "    for epoch in range(3):\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch[\"input_ids\"]\n",
    "            attention_mask = batch[\"attention_mask\"]\n",
    "\n",
    "            with torch.no_grad():\n",
    "                x = bert(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state\n",
    "                teacher_attn = teacher(x)\n",
    "                topk = torch.topk(teacher_attn[:, 0], k=1, dim=-1).indices.squeeze(-1)\n",
    "\n",
    "            loss = model(input_ids, attention_mask, topk, teacher_attn)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            print(\"Loss:\", loss.item())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
