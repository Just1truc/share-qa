{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import BertModel, BertTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeafEncoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, model_name='bert-base-uncased'):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.bert = BertModel.from_pretrained(model_name)\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "        for param in self.bert.parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "    def encode_query(self, input_ids):\n",
    "        return self.bert.embeddings.word_embeddings(input_ids)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        \n",
    "        # Add CLS token\n",
    "        cls_token_id = self.tokenizer.cls_token_id\n",
    "        cls_tokens = torch.full((input_ids.size(0), 1), cls_token_id, dtype=torch.long, device=input_ids.device)\n",
    "        input_ids = torch.cat([cls_tokens, input_ids], dim=1)\n",
    "        cls_mask = torch.ones((attention_mask.size(0), 1), dtype=attention_mask.dtype, device=attention_mask.device)\n",
    "        attention_mask = torch.cat([cls_mask, attention_mask], dim=1)\n",
    "        \n",
    "        # Process sequences\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask, output_attentions=True)\n",
    "        return outputs.last_hidden_state[:, 0, :], outputs.attentions[-1]\n",
    "\n",
    "\n",
    "class BinaryMerger(nn.Module):\n",
    "    \n",
    "    def __init__(self, d_model):\n",
    "        super().__init__()\n",
    "        self.merger = nn.Linear(2 * d_model, d_model)\n",
    "\n",
    "    def forward(self, left, right):\n",
    "        return self.merger(torch.cat([left, right], dim=-1))\n",
    "\n",
    "\n",
    "class HierarchicalBinaryTree(nn.Module):\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model : int,\n",
    "        tree_depth : int = 4,\n",
    "        k_sampling : int = 1,\n",
    "        train_seq_len : int = 512\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.tree_depth = tree_depth\n",
    "        self.leaf_encoder = LeafEncoder()\n",
    "        self.mergers = nn.ModuleList([BinaryMerger(d_model) for _ in range(2 ** tree_depth - 1)])\n",
    "        self.project_vectors = nn.Parameter(torch.randn(self.tree_depth, d_model))\n",
    "        self.train_seq_len : int = train_seq_len\n",
    "        self.k_sampling = k_sampling\n",
    "        \n",
    "        self.tree_cache = None\n",
    "        self.cached_tree_chunk_size = None\n",
    "        \n",
    "    @property\n",
    "    def modeling_capacity(self):\n",
    "        return self.train_seq_len * (2 ** self.tree_depth)\n",
    "        \n",
    "    def build_retrieval_tree(\n",
    "        self,\n",
    "        context_ids : torch.Tensor\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            context_ids (torch.Tensor): (B, L) input ids of the context. There should be not special tokens like CLS. \n",
    "            # answer_positions (torch.Tensor, optional): Allow for sparse building of the tree. Defaults to None. Reduce the building complexity by 2 (B, 1) TODO LATER\n",
    "        \"\"\"\n",
    "        k = 2 ** self.tree_depth\n",
    "        \n",
    "        assert (L % k == 0), f\"The chosen sequence size is not usable by the model. It should be divisable by {2 ** self.tree_depth}\"\n",
    "        \n",
    "        if L >  self.train_seq_len:\n",
    "            self.train_seq_len = L\n",
    "        \n",
    "        B, L = context_ids\n",
    "        \n",
    "        chunks = context_ids.view(B * k, L // k)\n",
    "        tree_levels = []\n",
    "        m : torch.Tensor = self.leaf_encoder(chunks).reshape(B, k, self.d_model) # (B * k, D)\n",
    "        D = self.d_model\n",
    "        \n",
    "        tree_levels.append(m)\n",
    "        for level in range(self.tree_depth):\n",
    "            level_l = m.shape[1]\n",
    "            new_stride = (level_l * D, D, D * level_l // 2, 1)\n",
    "            new_shape = (B, level_l // 2, 2, D)\n",
    "            final_shape = (B, level_l // 2, 2 * D)\n",
    "            m = self.mergers(torch.as_strided(m, new_shape, new_stride, final_shape).reshape(final_shape))\n",
    "            tree_levels.append(m)\n",
    "            \n",
    "        self.tree_cache = tree_levels\n",
    "        self.cached_tree_chunk_size = k\n",
    "        \n",
    "    def forward(\n",
    "        self,\n",
    "        query_ids : torch.Tensor,\n",
    "        answer_positions : torch.Tensor = None\n",
    "    ):\n",
    "        \"\"\"\n",
    "\n",
    "        Args:\n",
    "            query (torch.Tensor): (B, L)\n",
    "            answer_positions (torch.Tensor, optional): (B, L, K) K is has k-top softmax elements. If it's not None, the retreival loss will be calculated\n",
    "        \"\"\"\n",
    "        \n",
    "        assert (self.tree_cache != None), \"No tree to do retreival from?\"\n",
    "        \n",
    "        # Inference\n",
    "        if answer_positions == None:\n",
    "            ...\n",
    "            # Sample answers randomly at each level for k_sampling per token\n",
    "        \n",
    "            B, L = query_ids.shape\n",
    "            K = self.k_sampling\n",
    "            D = self.d_model\n",
    "            device = query_ids.device\n",
    "        \n",
    "            # Encode query tokens: (B, L, D)\n",
    "            with torch.no_grad():\n",
    "                queries = self.leaf_encoder.encode_query(query_ids)  # (B, L, D)\n",
    "            q_proj = self.q_proj(queries)  # (B, L, D)\n",
    "        \n",
    "            # Initialize: root index 0, repeated K times per token\n",
    "            current_indices = torch.zeros(B, L, K, dtype=torch.long, device=device)\n",
    "            selected_path = []\n",
    "            path_logits = []\n",
    "        \n",
    "            for l in range(self.tree_depth):\n",
    "                proj_q = q_proj.unsqueeze(2).expand(-1, -1, K, -1)  # (B, L, K, D)\n",
    "        \n",
    "                left_idx = current_indices * 2\n",
    "                right_idx = current_indices * 2 + 1  # both (B, L, K)\n",
    "        \n",
    "                left_nodes = self.tree_cache[l + 1].gather(\n",
    "                    1, left_idx.unsqueeze(-1).expand(-1, -1, -1, D))  # (B, L, K, D)\n",
    "                right_nodes = self.tree_cache[l + 1].gather(\n",
    "                    1, right_idx.unsqueeze(-1).expand(-1, -1, -1, D))  # (B, L, K, D)\n",
    "        \n",
    "                score_left = torch.sum(proj_q * left_nodes, dim=-1)     # (B, L, K)\n",
    "                score_right = torch.sum(proj_q * right_nodes, dim=-1)   # (B, L, K)\n",
    "        \n",
    "                logits = torch.stack([score_left, score_right], dim=-1)  # (B, L, K, 2)\n",
    "                path_logits.append(logits)\n",
    "        \n",
    "                # Expand to 2K candidates per token\n",
    "                all_scores = torch.cat([score_left, score_right], dim=-1)  # (B, L, 2K)\n",
    "                all_indices = torch.cat([left_idx, right_idx], dim=-1)     # (B, L, 2K)\n",
    "        \n",
    "                topk_scores, topk_idx = torch.topk(all_scores, k=K, dim=-1)       # (B, L, K)\n",
    "                new_indices = torch.gather(all_indices, dim=-1, index=topk_idx)   # (B, L, K)\n",
    "        \n",
    "                current_indices = new_indices\n",
    "                selected_path.append(current_indices)\n",
    "                \n",
    "            # === Final token-level prediction from selected chunks ===\n",
    "            max_chunk_len = self.cached_tree_chunk_size\n",
    "            top_token_scores = torch.zeros(B, L, K, max_chunk_len, device=query_ids.device)\n",
    "            \n",
    "            for b in range(B):\n",
    "                for l in range(L):\n",
    "                    query_token_id = query_ids[b, l].unsqueeze(0).unsqueeze(0)  # (1, 1)\n",
    "                    for k_id, chunk_idx in enumerate(current_indices[b, l]):  # current_indices: (B, L, K)\n",
    "                        # Select chunk\n",
    "                        chunk = self.context_chunks[chunk_idx]  # (1, chunk_len)\n",
    "            \n",
    "                        # Build BERT input: [CLS] query [SEP] chunk [SEP]\n",
    "                        input_ids = torch.cat([\n",
    "                            torch.tensor([[self.leaf_encoder.tokenizer.cls_token_id]], device=query_ids.device),\n",
    "                            query_token_id,\n",
    "                            torch.tensor([[self.leaf_encoder.tokenizer.sep_token_id]], device=query_ids.device),\n",
    "                            chunk,\n",
    "                            torch.tensor([[self.leaf_encoder.tokenizer.sep_token_id]], device=query_ids.device)\n",
    "                        ], dim=1)  # (1, T)\n",
    "            \n",
    "                        attention_mask = torch.ones_like(input_ids)\n",
    "            \n",
    "                        # Run through BERT\n",
    "                        with torch.no_grad():\n",
    "                            outputs = self.leaf_encoder.bert(input_ids=input_ids, attention_mask=attention_mask, output_attentions=True)\n",
    "            \n",
    "                        # Use last-layer attention from query token to chunk\n",
    "                        last_attn = outputs.attentions[-1]  # (layers, heads, 1, T)\n",
    "                        avg_attn = last_attn.mean(dim=(0, 1))  # (1, T)\n",
    "                        chunk_attn = avg_attn[0, 3:-1]  # skip CLS/query/SEP\n",
    "            \n",
    "                        # Save\n",
    "                        top_token_scores[b, l, k_id, :chunk_attn.size(0)] = chunk_attn\n",
    "            \n",
    "            # Final output: attention over chunk tokens for each query\n",
    "            # You could take argmax over last dim for final prediction\n",
    "            final_token_predictions = top_token_scores.argmax(dim=-1)  # (B, L, K)\n",
    "            # Assuming:\n",
    "            # - final_token_predictions: (B, L, K), local token index within chunk\n",
    "            # - current_indices: (B, L, K), chunk indices from tree traversal\n",
    "            # - self.cached_tree_chunk_size: scalar, number of tokens per chunk\n",
    "            \n",
    "            chunk_starts = current_indices * self.cached_tree_chunk_size  # (B, L, K)\n",
    "            global_token_predictions = chunk_starts + final_token_predictions  # (B, L, K)\n",
    "\n",
    "            return global_token_predictions\n",
    "            # return {\n",
    "            #     \"final_leaf_indices\": current_indices,  # (B, L, K)\n",
    "            #     \"path_logits\": path_logits,             # list of (B, L, K, 2)\n",
    "            #     \"selected_path\": selected_path          # list of (B, L, K)\n",
    "            # }\n",
    "        \n",
    "        # Training\n",
    "        else:\n",
    "            \n",
    "            # Infer the actual position of the k elements and from it the paths\n",
    "            # The chunk position of the answer positions is index // chunk_size. This give the index starting at 0 to n-1\n",
    "            \n",
    "            # leaf_indices = (answer_positions // self.cached_tree_chunk_size).clamp(max=2 ** self.tree_depth - 1)\n",
    "            # path_labels = torch.stack([(leaf_indices >> (self.tree_depth - l - 1)) & 1 for l in range(self.tree_depth)]) # (d, B, L, K)\n",
    "            \n",
    "            # # Encode queries (B, L) -> (B, L, D)\n",
    "            # queries = self.leaf_encoder.encode_query(query_ids)\n",
    "            \n",
    "            # # Project each query to each level using hadamar product (B, L, D) -> (B, L, d, D)\n",
    "            # query_projections = queries.unsqueeze(-2) * self.project_vectors \n",
    "            \n",
    "            # self.tree_cache (d, B, number of summaries at this node, D), path labels (d, B, L, K)\n",
    "            # traverse each level and select the summaries relevant to the input\n",
    "            \n",
    "            # Multiply the query projections and the merged_summaries selected (B, L, d, 1, D) * (B, L, d, K, D).swapaxes(-1, -2) \n",
    "            \n",
    "            B, L = query_ids.shape\n",
    "            D = self.d_model\n",
    "            device = query_ids.device\n",
    "\n",
    "            if answer_positions is None:\n",
    "                raise NotImplementedError(\"Sampling for inference is not yet implemented.\")\n",
    "\n",
    "            # === Step 1: Compute chunk indices from answer positions ===\n",
    "            # answer_positions: (B, L, K)\n",
    "            leaf_indices = (answer_positions // self.cached_tree_chunk_size).clamp(max=2 ** self.tree_depth - 1)  # (B, L, K)\n",
    "\n",
    "            # === Step 2: Compute binary path bits per level ===\n",
    "            # path_labels: (d, B, L, K), where each bit tells you to go left (0) or right (1)\n",
    "            path_labels = torch.stack([\n",
    "                (leaf_indices >> (self.tree_depth - l - 1)) & 1\n",
    "                for l in range(self.tree_depth)\n",
    "            ], dim=0)  # (d, B, L, K)\n",
    "\n",
    "            # === Step 3: Compute summary indices per level ===\n",
    "            # This gives the node index per level in the tree cache that corresponds to each top-K answer\n",
    "            level_indices = torch.zeros(self.tree_depth + 1, B, L, K := leaf_indices.shape[-1], dtype=torch.long, device=device)\n",
    "            level_indices[0] = 0  # Root is always index 0\n",
    "\n",
    "            for l in range(1, self.tree_depth + 1):\n",
    "                prev = level_indices[l - 1]  # (B, L, K)\n",
    "                bit = path_labels[l - 1]     # (B, L, K)\n",
    "                level_indices[l] = prev * 2 + bit  # Compute child index\n",
    "\n",
    "            # === Step 4: Encode the queries ===\n",
    "            queries = self.leaf_encoder.encode_query(query_ids)  # (B, L, D)\n",
    "            queries = queries.unsqueeze(-2) * self.project_vectors.view(self.tree_depth, 1, 1, D)  # (d, B, L, D)\n",
    "\n",
    "            # === Step 5: Select corresponding summaries from tree_cache using level_indices ===\n",
    "            # self.tree_cache: list of tensors per level: each (B, N_nodes, D)\n",
    "            selected_summaries = []\n",
    "            for l in range(1, self.tree_depth + 1):\n",
    "                cache = self.tree_cache[l]  # (B, N_nodes, D)\n",
    "                idx = level_indices[l]      # (B, L, K)\n",
    "\n",
    "                # Gather summaries at this level\n",
    "                gathered = torch.gather(\n",
    "                    cache.unsqueeze(1).expand(-1, L, -1, -1),  # (B, L, N_nodes, D)\n",
    "                    dim=2,\n",
    "                    index=idx.unsqueeze(-1).expand(-1, -1, -1, D)  # (B, L, K, D)\n",
    "                )  # (B, L, K, D)\n",
    "                selected_summaries.append(gathered)\n",
    "                \n",
    "            # === Step 6: Compute logits and loss ===\n",
    "            logits_per_level = []\n",
    "            loss = 0.0\n",
    "            d, B, L, K = path_labels.shape\n",
    "            \n",
    "            for l in range(self.tree_depth):\n",
    "                q_proj = queries[l]                           # (B, L, D)\n",
    "                summaries = selected_summaries[l]             # (B, L, K, D)\n",
    "                labels = path_labels[l]                       # (B, L, K)\n",
    "            \n",
    "                # Compute dot products: (B, L, K)\n",
    "                logits = torch.einsum('bld,blkd->blk', q_proj, summaries)\n",
    "            \n",
    "                # Reshape for cross-entropy: treat each (B, L) position as a batch element\n",
    "                logits_flat = logits.view(-1, K)              # (B * L, K)\n",
    "                labels_flat = labels.view(-1)                 # (B * L,)\n",
    "            \n",
    "                loss += F.cross_entropy(logits_flat, labels_flat)\n",
    "                logits_per_level.append(logits)\n",
    "            \n",
    "            return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import random\n",
    "\n",
    "class HotpotQADataset(Dataset):\n",
    "    def __init__(self, split=\"train\", tokenizer=None, max_length=512, num_queries=8):\n",
    "        self.data = load_dataset(\"hotpot_qa\", \"fullwiki\", split=split)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.num_queries = num_queries\n",
    "        self.vocab = list(tokenizer.get_vocab().values()) if tokenizer else list(range(30522))  # fallback to BERT vocab\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        context = item[\"context\"]  # list of (title, paragraph)\n",
    "        full_text = \" \".join([p for _, p in context])[:self.max_length * 4]  # truncate long inputs\n",
    "\n",
    "        encoding = self.tokenizer(\n",
    "            full_text,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        context_ids = encoding[\"input_ids\"].squeeze(0)  # (L,)\n",
    "        query_ids = torch.tensor(\n",
    "            random.choices(self.vocab, k=self.num_queries),\n",
    "            dtype=torch.long\n",
    "        )  # (num_queries,)\n",
    "\n",
    "        return {\n",
    "            \"context_ids\": context_ids,\n",
    "            \"query_ids\": query_ids\n",
    "        }\n",
    "\n",
    "def get_hotpotqa_dataloader(tokenizer, batch_size=8, split=\"train\"):\n",
    "    dataset = HotpotQADataset(split=split, tokenizer=tokenizer)\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HierarchicalBinaryTree(\n",
    "    d_model=768,\n",
    "    tree_depth=3,\n",
    "    k_sampling=1,\n",
    "    train_seq_len=32\n",
    ")\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "bert = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "269d152b2cc54eb0af60d8205aba9db0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/9.19k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e20cbf1199d942e8b97a54b9f729cdb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "hotpot_qa.py:   0%|          | 0.00/6.42k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "Loading hotpot_qa requires you to execute the dataset script in that repo on your local machine. Make sure you have read the code there to avoid malicious use, then set the option `trust_remote_code=True` to remove this error.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 60\u001b[39m\n\u001b[32m     56\u001b[39m         total_loss += loss.item()\n\u001b[32m     58\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m total_loss / \u001b[38;5;28mlen\u001b[39m(dataloader)\n\u001b[32m---> \u001b[39m\u001b[32m60\u001b[39m dataloader = \u001b[43mget_hotpotqa_dataloader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 42\u001b[39m, in \u001b[36mget_hotpotqa_dataloader\u001b[39m\u001b[34m(tokenizer, batch_size, split)\u001b[39m\n\u001b[32m     41\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34mget_hotpotqa_dataloader\u001b[39m(tokenizer, batch_size=\u001b[32m8\u001b[39m, split=\u001b[33m\"\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m     dataset = \u001b[43mHotpotQADataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m=\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     43\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m DataLoader(dataset, batch_size=batch_size, shuffle=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 8\u001b[39m, in \u001b[36mHotpotQADataset.__init__\u001b[39m\u001b[34m(self, split, tokenizer, max_length, num_queries)\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, split=\u001b[33m\"\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m\"\u001b[39m, tokenizer=\u001b[38;5;28;01mNone\u001b[39;00m, max_length=\u001b[32m512\u001b[39m, num_queries=\u001b[32m8\u001b[39m):\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m     \u001b[38;5;28mself\u001b[39m.data = \u001b[43mload_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mhotpot_qa\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfullwiki\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit\u001b[49m\u001b[43m=\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m     \u001b[38;5;28mself\u001b[39m.tokenizer = tokenizer\n\u001b[32m     10\u001b[39m     \u001b[38;5;28mself\u001b[39m.max_length = max_length\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.13/site-packages/datasets/load.py:2062\u001b[39m, in \u001b[36mload_dataset\u001b[39m\u001b[34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, keep_in_memory, save_infos, revision, token, streaming, num_proc, storage_options, trust_remote_code, **config_kwargs)\u001b[39m\n\u001b[32m   2057\u001b[39m verification_mode = VerificationMode(\n\u001b[32m   2058\u001b[39m     (verification_mode \u001b[38;5;129;01mor\u001b[39;00m VerificationMode.BASIC_CHECKS) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m save_infos \u001b[38;5;28;01melse\u001b[39;00m VerificationMode.ALL_CHECKS\n\u001b[32m   2059\u001b[39m )\n\u001b[32m   2061\u001b[39m \u001b[38;5;66;03m# Create a dataset builder\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2062\u001b[39m builder_instance = \u001b[43mload_dataset_builder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2063\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2064\u001b[39m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2065\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2066\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2067\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2068\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2069\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2070\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2071\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2072\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2073\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2074\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2075\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_require_default_config_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   2076\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2077\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2079\u001b[39m \u001b[38;5;66;03m# Return iterable dataset in case of streaming\u001b[39;00m\n\u001b[32m   2080\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m streaming:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.13/site-packages/datasets/load.py:1782\u001b[39m, in \u001b[36mload_dataset_builder\u001b[39m\u001b[34m(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, token, storage_options, trust_remote_code, _require_default_config_name, **config_kwargs)\u001b[39m\n\u001b[32m   1780\u001b[39m     download_config = download_config.copy() \u001b[38;5;28;01mif\u001b[39;00m download_config \u001b[38;5;28;01melse\u001b[39;00m DownloadConfig()\n\u001b[32m   1781\u001b[39m     download_config.storage_options.update(storage_options)\n\u001b[32m-> \u001b[39m\u001b[32m1782\u001b[39m dataset_module = \u001b[43mdataset_module_factory\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1783\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1784\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1785\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1786\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1787\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1788\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1789\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1790\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1791\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_require_default_config_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_require_default_config_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1792\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_require_custom_configs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mbool\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1793\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1794\u001b[39m \u001b[38;5;66;03m# Get dataset builder class from the processing script\u001b[39;00m\n\u001b[32m   1795\u001b[39m builder_kwargs = dataset_module.builder_kwargs\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.13/site-packages/datasets/load.py:1664\u001b[39m, in \u001b[36mdataset_module_factory\u001b[39m\u001b[34m(path, revision, download_config, download_mode, dynamic_modules_path, data_dir, data_files, cache_dir, trust_remote_code, _require_default_config_name, _require_custom_configs, **download_kwargs)\u001b[39m\n\u001b[32m   1659\u001b[39m                 \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1660\u001b[39m                     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\n\u001b[32m   1661\u001b[39m                         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCouldn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt find any data file at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrelative_to_absolute_path(path)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1662\u001b[39m                         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCouldn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt find \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m on the Hugging Face Hub either: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(e1).\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me1\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   1663\u001b[39m                     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1664\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m e1 \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1665\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m trust_remote_code:\n\u001b[32m   1666\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\n\u001b[32m   1667\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCouldn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt find a dataset script at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrelative_to_absolute_path(combined_path)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m or any data file in the same directory.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1668\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.13/site-packages/datasets/load.py:1614\u001b[39m, in \u001b[36mdataset_module_factory\u001b[39m\u001b[34m(path, revision, download_config, download_mode, dynamic_modules_path, data_dir, data_files, cache_dir, trust_remote_code, _require_default_config_name, _require_custom_configs, **download_kwargs)\u001b[39m\n\u001b[32m   1605\u001b[39m             \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m   1606\u001b[39m     \u001b[38;5;66;03m# Otherwise we must use the dataset script if the user trusts it\u001b[39;00m\n\u001b[32m   1607\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mHubDatasetModuleFactoryWithScript\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1608\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1609\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcommit_hash\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1610\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1611\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1612\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdynamic_modules_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdynamic_modules_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1613\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m-> \u001b[39m\u001b[32m1614\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1615\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m EntryNotFoundError:\n\u001b[32m   1616\u001b[39m     \u001b[38;5;66;03m# Use the infos from the parquet export except in some cases:\u001b[39;00m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m data_dir \u001b[38;5;129;01mor\u001b[39;00m data_files \u001b[38;5;129;01mor\u001b[39;00m (revision \u001b[38;5;129;01mand\u001b[39;00m revision != \u001b[33m\"\u001b[39m\u001b[33mmain\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.13/site-packages/datasets/load.py:1277\u001b[39m, in \u001b[36mHubDatasetModuleFactoryWithScript.get_module\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1266\u001b[39m         _create_importable_file(\n\u001b[32m   1267\u001b[39m             local_path=local_path,\n\u001b[32m   1268\u001b[39m             local_imports=local_imports,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1274\u001b[39m             download_mode=\u001b[38;5;28mself\u001b[39m.download_mode,\n\u001b[32m   1275\u001b[39m         )\n\u001b[32m   1276\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1277\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1278\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mLoading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m requires you to execute the dataset script in that\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1279\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m repo on your local machine. Make sure you have read the code there to avoid malicious use, then\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1280\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m set the option `trust_remote_code=True` to remove this error.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1281\u001b[39m         )\n\u001b[32m   1282\u001b[39m _check_library_imports(name=\u001b[38;5;28mself\u001b[39m.name, library_imports=library_imports)\n\u001b[32m   1283\u001b[39m module_path, \u001b[38;5;28mhash\u001b[39m = _load_importable_file(\n\u001b[32m   1284\u001b[39m     dynamic_modules_path=dynamic_modules_path,\n\u001b[32m   1285\u001b[39m     module_namespace=\u001b[33m\"\u001b[39m\u001b[33mdatasets\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1286\u001b[39m     subdirectory_name=\u001b[38;5;28mhash\u001b[39m,\n\u001b[32m   1287\u001b[39m     name=\u001b[38;5;28mself\u001b[39m.name,\n\u001b[32m   1288\u001b[39m )\n",
      "\u001b[31mValueError\u001b[39m: Loading hotpot_qa requires you to execute the dataset script in that repo on your local machine. Make sure you have read the code there to avoid malicious use, then set the option `trust_remote_code=True` to remove this error."
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "def generate_topk_labels(query_ids, context_ids, k=5):\n",
    "    \"\"\"\n",
    "    For each token in the query, compute the top-k attended context token positions.\n",
    "    Returns: (B, Lq, K) long tensor of token indices in the context\n",
    "    \"\"\"\n",
    "    B, Lq = query_ids.shape\n",
    "    _, Lc = context_ids.shape\n",
    "\n",
    "    input_ids = torch.cat([query_ids, context_ids], dim=1)  # (B, Lq + Lc)\n",
    "    attention_mask = (input_ids != tokenizer.pad_token_id).long()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.leaf_encoder.bert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            output_attentions=True\n",
    "        )\n",
    "        attn = outputs.attentions[-1]  # (layers, heads, B, T, T)\n",
    "        avg_attn = attn.mean(dim=(0, 1))  # (B, T, T)\n",
    "\n",
    "        # Only look at attention from query tokens to context tokens\n",
    "        query_to_context_attn = avg_attn[:, :Lq, Lq:]  # (B, Lq, Lc)\n",
    "\n",
    "        # Get top-k positions per query token\n",
    "        topk_indices = torch.topk(query_to_context_attn, k=k, dim=-1).indices  # (B, Lq, K)\n",
    "\n",
    "    return topk_indices  # (B, Lq, K) — token-level query-dependent top-k\n",
    "\n",
    "\n",
    "def train_one_epoch(model, dataloader, optimizer, k=5):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in tqdm(dataloader):\n",
    "        context_ids = batch[\"context_ids\"]  # (B, L)\n",
    "        query_ids = batch[\"query_ids\"]      # (B, Lq)\n",
    "        attention_mask = (context_ids != tokenizer.pad_token_id).long()\n",
    "\n",
    "        # Build tree from context\n",
    "        model.build_retrieval_tree(context_ids)\n",
    "\n",
    "        # Get pseudo-labels using BERT attention\n",
    "        topk_indices = generate_topk_labels(query_ids, context_ids, attention_mask, k=k)  # (B, k)\n",
    "\n",
    "        # Broadcast labels to match token-level (Lq) inputs\n",
    "        topk_labels = topk_indices.unsqueeze(1).expand(-1, query_ids.shape[1], -1)  # (B, Lq, k)\n",
    "\n",
    "        # Forward pass\n",
    "        loss = model(query_ids, answer_positions=topk_labels)  # output[\"loss\"], etc.\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "dataloader = get_hotpotqa_dataloader(tokenizer, trust_remote_code=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
