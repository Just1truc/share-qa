{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jupiter Notebook of all the parsings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: faiss-cpu in /home/ostentatoire/.local/lib/python3.13/site-packages (1.10.0)\n",
      "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/lib64/python3.13/site-packages (from faiss-cpu) (1.26.4)\n",
      "Requirement already satisfied: packaging in /usr/lib/python3.13/site-packages (from faiss-cpu) (24.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: ijson in /home/ostentatoire/.local/lib/python3.13/site-packages (3.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install faiss-cpu\n",
    "%pip install ijson\n",
    "\n",
    "from universal_parser import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mount the drive on collab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MediaSum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://huggingface.co/datasets/ccdv/mediasum/resolve/main/train_data.zip\n",
    "!unzip train_data.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = MediaSumParser()\n",
    "\n",
    "parser.convert(\n",
    "    dataset_path    = \"./train_data.txt\",\n",
    "    output_path     = \"/content/drive/MyDrive/media_sum.csv\",\n",
    "    to_process      = 463596\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SODA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://huggingface.co/datasets/allenai/soda/resolve/main/train.parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To_process: 1191582\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/content/drive/MyDrive/soda.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m parser = SODA()\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[43mparser\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdataset_path\u001b[49m\u001b[43m    \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m./train.parquet\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_path\u001b[49m\u001b[43m     \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/content/drive/MyDrive/soda.csv\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mread_format\u001b[49m\u001b[43m     \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mparquet\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m      7\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Celia/SHARE/universal_parser.py:197\u001b[39m, in \u001b[36mUniversalParser.convert\u001b[39m\u001b[34m(self, dataset_path, output_path, start_unit, to_process, read_format)\u001b[39m\n\u001b[32m    193\u001b[39m to_process = to_process \u001b[38;5;28;01mif\u001b[39;00m to_process != \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.process_size(dataset_path, read_format)\n\u001b[32m    195\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mTo_process:\u001b[39m\u001b[33m\"\u001b[39m, to_process)\n\u001b[32m--> \u001b[39m\u001b[32m197\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mw+\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m    198\u001b[39m     writer = csv.DictWriter(f, fieldnames=fieldnames)\n\u001b[32m    199\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m start_unit == \u001b[32m0\u001b[39m:\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: '/content/drive/MyDrive/soda.csv'"
     ]
    }
   ],
   "source": [
    "parser = SODA()\n",
    "\n",
    "parser.convert(\n",
    "    dataset_path    = \"./train.parquet\",\n",
    "    output_path     = \"/content/drive/MyDrive/soda.csv\",\n",
    "    read_format     = \"parquet\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SamSUM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://huggingface.co/datasets/knkarthick/samsum/resolve/main/train.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = SamSUM()\n",
    "\n",
    "parser.convert(\n",
    "    dataset_path    = \"./train.csv\",\n",
    "    output_path     = \"/content/drive/MyDrive/samsum.csv\",\n",
    "    read_format     = \"csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CSVs to Parquet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: datasets in /home/ostentatoire/.local/lib/python3.13/site-packages (3.5.0)\n",
      "Requirement already satisfied: filelock in /usr/lib/python3.13/site-packages (from datasets) (3.15.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/lib64/python3.13/site-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /home/ostentatoire/.local/lib/python3.13/site-packages (from datasets) (18.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/ostentatoire/.local/lib/python3.13/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /home/ostentatoire/.local/lib/python3.13/site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/lib/python3.13/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /usr/lib/python3.13/site-packages (from datasets) (4.66.5)\n",
      "Requirement already satisfied: xxhash in /home/ostentatoire/.local/lib/python3.13/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /home/ostentatoire/.local/lib/python3.13/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /home/ostentatoire/.local/lib/python3.13/site-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.10.0)\n",
      "Requirement already satisfied: aiohttp in /home/ostentatoire/.local/lib/python3.13/site-packages (from datasets) (3.11.11)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /home/ostentatoire/.local/lib/python3.13/site-packages (from datasets) (0.30.1)\n",
      "Requirement already satisfied: packaging in /usr/lib/python3.13/site-packages (from datasets) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/lib64/python3.13/site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/ostentatoire/.local/lib/python3.13/site-packages (from aiohttp->datasets) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/ostentatoire/.local/lib/python3.13/site-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/lib/python3.13/site-packages (from aiohttp->datasets) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/ostentatoire/.local/lib/python3.13/site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/ostentatoire/.local/lib/python3.13/site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/ostentatoire/.local/lib/python3.13/site-packages (from aiohttp->datasets) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/ostentatoire/.local/lib/python3.13/site-packages (from aiohttp->datasets) (1.18.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ostentatoire/.local/lib/python3.13/site-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/lib/python3.13/site-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3.13/site-packages (from requests>=2.32.2->datasets) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3.13/site-packages (from requests>=2.32.2->datasets) (1.26.20)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/lib/python3.13/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/lib/python3.13/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/ostentatoire/.local/lib/python3.13/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3.13/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_paths = [\n",
    "    \"datasets/MediaSum/parsed.csv\",\n",
    "    \"datasets/SamSUM/parsed.csv\",\n",
    "    \"datasets/SODA/parsed.csv\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: datasets/MediaSum/parsed.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating parquet from Arrow format: 100%|██████████| 50/50 [00:02<00:00, 23.60ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 50/50 [00:02<00:00, 21.72ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 50/50 [00:03<00:00, 15.32ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 50/50 [00:03<00:00, 13.50ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 50/50 [00:02<00:00, 17.46ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 50/50 [00:02<00:00, 18.75ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: datasets/SamSUM/parsed.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating parquet from Arrow format: 100%|██████████| 15/15 [00:00<00:00, 233.48ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: datasets/SODA/parsed.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating parquet from Arrow format: 100%|██████████| 50/50 [00:00<00:00, 190.01ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 50/50 [00:00<00:00, 197.44ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 50/50 [00:00<00:00, 193.17ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 50/50 [00:00<00:00, 192.66ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 50/50 [00:00<00:00, 189.84ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 50/50 [00:00<00:00, 191.25ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 50/50 [00:00<00:00, 185.68ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 50/50 [00:00<00:00, 190.16ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 50/50 [00:00<00:00, 190.34ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 50/50 [00:00<00:00, 187.43ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 50/50 [00:00<00:00, 198.69ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 50/50 [00:00<00:00, 182.56ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 50/50 [00:00<00:00, 196.29ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 50/50 [00:00<00:00, 193.67ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 50/50 [00:00<00:00, 185.61ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 50/50 [00:00<00:00, 160.68ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 50/50 [00:00<00:00, 193.04ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 50/50 [00:00<00:00, 196.27ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 50/50 [00:00<00:00, 178.53ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 50/50 [00:00<00:00, 186.69ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 50/50 [00:00<00:00, 191.81ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 50/50 [00:00<00:00, 189.32ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 50/50 [00:00<00:00, 172.61ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 42/42 [00:00<00:00, 192.75ba/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from datasets import Dataset, concatenate_datasets\n",
    "import pyarrow as pa\n",
    "\n",
    "parquet_output_dir = \"sharded_parquet\"\n",
    "batch_size = 50000\n",
    "os.makedirs(parquet_output_dir, exist_ok=True)\n",
    "\n",
    "def write_chunk_to_parquet(chunk_df, shard_idx):\n",
    "    dataset = Dataset.from_pandas(chunk_df, preserve_index=False)\n",
    "    dataset.to_parquet(f\"{parquet_output_dir}/shard_{shard_idx:05d}.parquet\")\n",
    "\n",
    "shard_idx = 0\n",
    "for csv_file in datasets_paths:\n",
    "\n",
    "    print(f\"Processing: {csv_file}\")\n",
    "    csv_path = os.path.join(csv_file)\n",
    "    \n",
    "    for chunk in pd.read_csv(csv_path, chunksize=batch_size):\n",
    "        write_chunk_to_parquet(chunk, shard_idx)\n",
    "        shard_idx += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Push to Huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting ipywidgets\n",
      "  Downloading ipywidgets-8.1.5-py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: comm>=0.1.3 in /home/ostentatoire/.local/lib/python3.13/site-packages (from ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /home/ostentatoire/.local/lib/python3.13/site-packages (from ipywidgets) (9.0.2)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /home/ostentatoire/.local/lib/python3.13/site-packages (from ipywidgets) (5.14.3)\n",
      "Collecting widgetsnbextension~=4.0.12 (from ipywidgets)\n",
      "  Downloading widgetsnbextension-4.0.13-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting jupyterlab-widgets~=3.0.12 (from ipywidgets)\n",
      "  Downloading jupyterlab_widgets-3.0.13-py3-none-any.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: decorator in /usr/lib/python3.13/site-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: ipython-pygments-lexers in /home/ostentatoire/.local/lib/python3.13/site-packages (from ipython>=6.1.0->ipywidgets) (1.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /home/ostentatoire/.local/lib/python3.13/site-packages (from ipython>=6.1.0->ipywidgets) (0.19.2)\n",
      "Requirement already satisfied: matplotlib-inline in /home/ostentatoire/.local/lib/python3.13/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.7)\n",
      "Requirement already satisfied: pexpect>4.3 in /usr/lib/python3.13/site-packages (from ipython>=6.1.0->ipywidgets) (4.9.0)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in /usr/lib/python3.13/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.41)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /usr/lib/python3.13/site-packages (from ipython>=6.1.0->ipywidgets) (2.18.0)\n",
      "Requirement already satisfied: stack_data in /home/ostentatoire/.local/lib/python3.13/site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /home/ostentatoire/.local/lib/python3.13/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.4)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /usr/lib/python3.13/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /usr/lib/python3.13/site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.13)\n",
      "Requirement already satisfied: executing>=1.2.0 in /home/ostentatoire/.local/lib/python3.13/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (2.2.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /home/ostentatoire/.local/lib/python3.13/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (3.0.0)\n",
      "Requirement already satisfied: pure-eval in /home/ostentatoire/.local/lib/python3.13/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (0.2.3)\n",
      "Downloading ipywidgets-8.1.5-py3-none-any.whl (139 kB)\n",
      "Downloading jupyterlab_widgets-3.0.13-py3-none-any.whl (214 kB)\n",
      "Downloading widgetsnbextension-4.0.13-py3-none-any.whl (2.3 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m574.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m1m563.6 kB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: widgetsnbextension, jupyterlab-widgets, ipywidgets\n",
      "Successfully installed ipywidgets-8.1.5 jupyterlab-widgets-3.0.13 widgetsnbextension-4.0.13\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pushing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating parquet from Arrow format: 100%|██████████| 137/137 [00:41<00:00,  3.30ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 137/137 [00:22<00:00,  6.20ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 137/137 [00:02<00:00, 50.83ba/s] \n",
      "Creating parquet from Arrow format: 100%|██████████| 137/137 [00:01<00:00, 128.11ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 137/137 [00:01<00:00, 130.77ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 137/137 [00:00<00:00, 146.99ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 137/137 [00:00<00:00, 156.37ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 137/137 [00:00<00:00, 165.32ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 137/137 [00:00<00:00, 158.76ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 137/137 [00:00<00:00, 164.48ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 137/137 [00:00<00:00, 165.75ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 11/11 [17:18<00:00, 94.43s/it]\n",
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pushed\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "streamable_dataset = load_dataset(\"parquet\", data_files=f\"{parquet_output_dir}/*.parquet\", split=\"train\")\n",
    "streamable_dataset.push_to_hub(\"JustinDuc/MultiDomain-QADialog\", token=\"[YOUR TOKEN]\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
