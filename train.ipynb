{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yQr6ynHJe3U6"
      },
      "source": [
        "# Train SHARE Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ciCuk0dHe3U8"
      },
      "source": [
        "## Train SAUTE model with MLM Loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c2pm69a2e3U8"
      },
      "source": [
        "DO NOT RUN IN LOCAL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LAh4vZize3U9",
        "outputId": "0a3787c7-f302-485a-cd68-f9574a2e78d3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  5117  100  5117    0     0  11185      0 --:--:-- --:--:-- --:--:-- 11196\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 17655  100 17655    0     0  37186      0 --:--:-- --:--:-- --:--:-- 37168\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  1290  100  1290    0     0   2853      0 --:--:-- --:--:-- --:--:--  2860\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  2301  100  2301    0     0   5701      0 --:--:-- --:--:-- --:--:--  5695\n"
          ]
        }
      ],
      "source": [
        "!mkdir sources\n",
        "!curl https://raw.githubusercontent.com/Just1truc/share-qa/refs/heads/main/sources/datasets.py -o sources/datasets.py\n",
        "!curl https://raw.githubusercontent.com/Just1truc/share-qa/refs/heads/main/sources/saute_model.py -o sources/saute_model.py\n",
        "!curl https://raw.githubusercontent.com/Just1truc/share-qa/refs/heads/main/sources/saute_config.py -o sources/saute_config.py\n",
        "!curl https://raw.githubusercontent.com/Just1truc/share-qa/refs/heads/main/sources/mlm_logger.py -o sources/mlm_logger.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4QsHIaNke3U-"
      },
      "source": [
        "You might need to restart session to actualize jupiter notebook env here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sLp4N_Spe3U-"
      },
      "source": [
        "### Installing dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kLA8gsPee3U_",
        "outputId": "46b1b419-92a8-495b-9a53-5aa57d8215a8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting flash-attn==1.0.8\n",
            "  Downloading flash_attn-1.0.8.tar.gz (2.0 MB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.0 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m83.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m53.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from flash-attn==1.0.8) (2.6.0+cu124)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (from flash-attn==1.0.8) (0.8.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from flash-attn==1.0.8) (24.2)\n",
            "Collecting ninja (from flash-attn==1.0.8)\n",
            "  Downloading ninja-1.11.1.4-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn==1.0.8) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn==1.0.8) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn==1.0.8) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn==1.0.8) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn==1.0.8) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch->flash-attn==1.0.8)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch->flash-attn==1.0.8)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch->flash-attn==1.0.8)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch->flash-attn==1.0.8)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch->flash-attn==1.0.8)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch->flash-attn==1.0.8)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch->flash-attn==1.0.8)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch->flash-attn==1.0.8)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch->flash-attn==1.0.8)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn==1.0.8) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn==1.0.8) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn==1.0.8) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch->flash-attn==1.0.8)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn==1.0.8) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn==1.0.8) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->flash-attn==1.0.8) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->flash-attn==1.0.8) (3.0.2)\n",
            "Downloading ninja-1.11.1.4-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (422 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m422.8/422.8 kB\u001b[0m \u001b[31m36.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m69.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m37.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m47.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m103.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: flash-attn\n",
            "  Building wheel for flash-attn (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for flash-attn: filename=flash_attn-1.0.8-cp311-cp311-linux_x86_64.whl size=73033174 sha256=72c7ce9a554be1f00cdeabb5058be3c0b4110358b9ccf86e79419d226dab3f55\n",
            "  Stored in directory: /root/.cache/pip/wheels/0f/08/28/936e0ba69ce518ba11fae23fad0045432fedce7e8c63c20807\n",
            "Successfully built flash-attn\n",
            "Installing collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, ninja, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, flash-attn\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed flash-attn-1.0.8 ninja-1.11.1.4 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
            "Collecting datasets\n",
            "  Downloading datasets-3.5.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.12.0,>=2023.1.0 (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.30.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.19.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Downloading datasets-3.5.0-py3-none-any.whl (491 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.2/491.2 kB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.2\n",
            "    Uninstalling fsspec-2025.3.2:\n",
            "      Successfully uninstalled fsspec-2025.3.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2024.12.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.5.0 dill-0.3.8 fsspec-2024.12.0 multiprocess-0.70.16 xxhash-3.5.0\n",
            "Collecting torchinfo\n",
            "  Downloading torchinfo-1.8.0-py3-none-any.whl.metadata (21 kB)\n",
            "Downloading torchinfo-1.8.0-py3-none-any.whl (23 kB)\n",
            "Installing collected packages: torchinfo\n",
            "Successfully installed torchinfo-1.8.0\n"
          ]
        }
      ],
      "source": [
        "%pip install flash-attn==1.0.8 --no-build-isolation\n",
        "%pip install -U transformers\n",
        "%pip install datasets\n",
        "%pip install torchinfo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ByIaeMqIe3VA"
      },
      "source": [
        "#### Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "UMR3g0x7e3VA"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "from transformers         import Trainer, TrainingArguments, BertConfig, BertForMaskedLM, BertTokenizerFast\n",
        "from sources.saute_model  import UtteranceEmbedings\n",
        "from sources.saute_config import SAUTEConfig\n",
        "from sources.datasets     import SAUTEDataset\n",
        "from sources.mlm_logger   import WandbPredictionLoggerCallback\n",
        "from torchinfo            import summary"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Accuracy Metric"
      ],
      "metadata": {
        "id": "4gwugq-z7xxL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_masked_accuracy(logits, labels):\n",
        "    # logits: [batch_size, seq_len, vocab_size]\n",
        "    # labels: [batch_size, seq_len]\n",
        "\n",
        "    preds = torch.argmax(logits, dim=-1)  # [batch_size, seq_len]\n",
        "\n",
        "    # Only consider masked positions (labels != -100)\n",
        "    mask = labels != -100\n",
        "\n",
        "    # Count correct predictions\n",
        "    correct = (preds == labels) & mask\n",
        "    accuracy = correct.sum().float() / mask.sum()\n",
        "\n",
        "    return accuracy.item()\n",
        "\n",
        "def compute_metrics(eval_preds):\n",
        "    logits, labels = eval_preds\n",
        "    logits = torch.tensor(logits)\n",
        "    labels = torch.tensor(labels)\n",
        "    acc = compute_masked_accuracy(logits, labels)\n",
        "    return {\"masked_accuracy\": acc}"
      ],
      "metadata": {
        "id": "1ZJxDIOy70GC"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qRFLXLPd-wHI"
      },
      "source": [
        "#### Load Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "BG13ZCMw-wHJ"
      },
      "outputs": [],
      "source": [
        "train_dataset = SAUTEDataset(split=\"train\", dialog_format=\"edu\")\n",
        "eval_dataset = SAUTEDataset(split=\"test\", dialog_format=\"edu\")\n",
        "\n",
        "from torch.utils.data import Subset\n",
        "import random\n",
        "\n",
        "subset_size = 25\n",
        "indices = random.sample(range(len(eval_dataset)), subset_size)\n",
        "test_dataset = Subset(eval_dataset, indices)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_vMjqbeRe3VC"
      },
      "source": [
        "#### Load Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "6_qd8jbUe3VC"
      },
      "outputs": [],
      "source": [
        "model_config = SAUTEConfig(\n",
        "    num_attention_heads = 12,\n",
        "    num_hidden_layers   = 2\n",
        ")\n",
        "model = UtteranceEmbedings(model_config).to(\"cuda:0\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fryRRxI6e3VD"
      },
      "source": [
        "#### Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Es43sx9mfEXz"
      },
      "outputs": [],
      "source": [
        "fixed_batch = train_dataset[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Model Statistics"
      ],
      "metadata": {
        "id": "nFqDoujU36wh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class ModelWrapper(torch.nn.Module):\n",
        "    def __init__(self, model):\n",
        "        super().__init__()\n",
        "        self.model = model\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        return self.model(input_ids=input_ids, attention_mask=attention_mask, speaker_names=fixed_batch[\"speaker_names\"])\n",
        "\n",
        "wrapped_model = ModelWrapper(model).to(\"cuda:0\")\n",
        "\n",
        "summary(\n",
        "    wrapped_model,\n",
        "    input_data=(fixed_batch[\"input_ids\"].to(\"cuda:0\"), fixed_batch[\"attention_mask\"].to(\"cuda:0\")),\n",
        "    col_names=[\"input_size\", \"output_size\", \"num_params\", \"mult_adds\"],\n",
        "    depth=3,\n",
        "    verbose=1\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ET8tmLzj39Fb",
        "outputId": "d8121b10-9cc2-49ec-ae8b-713f1709faf0"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=====================================================================================================================================================================\n",
            "Layer (type:depth-idx)                                            Input Shape               Output Shape              Param #                   Mult-Adds\n",
            "=====================================================================================================================================================================\n",
            "ModelWrapper                                                      [6, 128]                  [6, 128, 30522]           --                        --\n",
            "├─UtteranceEmbedings: 1-1                                         --                        [6, 128, 30522]           --                        --\n",
            "│    └─HSauteUnit: 2-1                                            --                        --                        --                        --\n",
            "│    │    └─Embedding: 3-1                                        [6, 128]                  [6, 128, 768]             23,440,896                140,645,376\n",
            "│    │    └─Embedding: 3-2                                        [6, 128]                  [6, 128, 768]             393,216                   2,359,296\n",
            "│    │    └─ModuleList: 3-3                                       --                        --                        29,126,656                --\n",
            "│    └─Linear: 2-2                                                [6, 128, 768]             [6, 128, 30522]           23,471,418                140,828,508\n",
            "=====================================================================================================================================================================\n",
            "Total params: 76,432,186\n",
            "Trainable params: 76,432,186\n",
            "Non-trainable params: 0\n",
            "Total mult-adds (Units.GIGABYTES): 141.77\n",
            "=====================================================================================================================================================================\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 301.26\n",
            "Params size (MB): 305.73\n",
            "Estimated Total Size (MB): 607.01\n",
            "=====================================================================================================================================================================\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "=====================================================================================================================================================================\n",
              "Layer (type:depth-idx)                                            Input Shape               Output Shape              Param #                   Mult-Adds\n",
              "=====================================================================================================================================================================\n",
              "ModelWrapper                                                      [6, 128]                  [6, 128, 30522]           --                        --\n",
              "├─UtteranceEmbedings: 1-1                                         --                        [6, 128, 30522]           --                        --\n",
              "│    └─HSauteUnit: 2-1                                            --                        --                        --                        --\n",
              "│    │    └─Embedding: 3-1                                        [6, 128]                  [6, 128, 768]             23,440,896                140,645,376\n",
              "│    │    └─Embedding: 3-2                                        [6, 128]                  [6, 128, 768]             393,216                   2,359,296\n",
              "│    │    └─ModuleList: 3-3                                       --                        --                        29,126,656                --\n",
              "│    └─Linear: 2-2                                                [6, 128, 768]             [6, 128, 30522]           23,471,418                140,828,508\n",
              "=====================================================================================================================================================================\n",
              "Total params: 76,432,186\n",
              "Trainable params: 76,432,186\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (Units.GIGABYTES): 141.77\n",
              "=====================================================================================================================================================================\n",
              "Input size (MB): 0.01\n",
              "Forward/backward pass size (MB): 301.26\n",
              "Params size (MB): 305.73\n",
              "Estimated Total Size (MB): 607.01\n",
              "====================================================================================================================================================================="
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mIp7ogUv-wHN"
      },
      "source": [
        "#### Init Training necessities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 236
        },
        "id": "czFdi5L1-wHN",
        "outputId": "a385edb8-783f-48a0-9772-ea11e6b0499a"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'train_dataset' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-474796f0879f>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfixed_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBertTokenizerFast\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"bert-base-uncased\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Initialize the callback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m wandb_logger_callback = WandbPredictionLoggerCallback(\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train_dataset' is not defined"
          ]
        }
      ],
      "source": [
        "fixed_batch = train_dataset[1]\n",
        "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "# Initialize the callback\n",
        "wandb_logger_callback = WandbPredictionLoggerCallback(\n",
        "    fixed_batch=fixed_batch,\n",
        "    tokenizer=tokenizer,\n",
        "    log_every_steps=50\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "H5XrE4mUe3VD",
        "outputId": "c9f9b739-4275-48b2-b706-9029c3b6c87d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-6e24e14dbab4>:18: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjustinduc\u001b[0m (\u001b[33mjustinduc-epitech\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.9"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250420_070920-9mf37qm2</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/justinduc-epitech/huggingface/runs/9mf37qm2' target=\"_blank\">h-saute-mlm-76m-0.0.1</a></strong> to <a href='https://wandb.ai/justinduc-epitech/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/justinduc-epitech/huggingface' target=\"_blank\">https://wandb.ai/justinduc-epitech/huggingface</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/justinduc-epitech/huggingface/runs/9mf37qm2' target=\"_blank\">https://wandb.ai/justinduc-epitech/huggingface/runs/9mf37qm2</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='11941' max='1506100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [  11941/1506100 34:20 < 71:37:38, 5.79 it/s, Epoch 0.01/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Masked Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>5.834800</td>\n",
              "      <td>6.220988</td>\n",
              "      <td>0.069164</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>5.372000</td>\n",
              "      <td>6.107636</td>\n",
              "      <td>0.075501</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>450</td>\n",
              "      <td>5.511800</td>\n",
              "      <td>5.931244</td>\n",
              "      <td>0.110283</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>5.670200</td>\n",
              "      <td>5.749789</td>\n",
              "      <td>0.107784</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>750</td>\n",
              "      <td>5.345600</td>\n",
              "      <td>5.531967</td>\n",
              "      <td>0.131200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>5.889700</td>\n",
              "      <td>5.668138</td>\n",
              "      <td>0.112462</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1050</td>\n",
              "      <td>5.650500</td>\n",
              "      <td>5.646456</td>\n",
              "      <td>0.131356</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1200</td>\n",
              "      <td>4.909500</td>\n",
              "      <td>5.522082</td>\n",
              "      <td>0.127055</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1350</td>\n",
              "      <td>5.310300</td>\n",
              "      <td>5.577678</td>\n",
              "      <td>0.130758</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>5.181300</td>\n",
              "      <td>5.491559</td>\n",
              "      <td>0.130045</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1650</td>\n",
              "      <td>4.953200</td>\n",
              "      <td>5.534338</td>\n",
              "      <td>0.163205</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1800</td>\n",
              "      <td>4.981400</td>\n",
              "      <td>5.237103</td>\n",
              "      <td>0.170338</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1950</td>\n",
              "      <td>4.730400</td>\n",
              "      <td>5.099463</td>\n",
              "      <td>0.213092</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2100</td>\n",
              "      <td>4.955900</td>\n",
              "      <td>4.870996</td>\n",
              "      <td>0.209023</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2250</td>\n",
              "      <td>4.913200</td>\n",
              "      <td>5.226933</td>\n",
              "      <td>0.198804</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2400</td>\n",
              "      <td>4.614000</td>\n",
              "      <td>4.918139</td>\n",
              "      <td>0.227401</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2550</td>\n",
              "      <td>4.560700</td>\n",
              "      <td>4.931507</td>\n",
              "      <td>0.205318</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2700</td>\n",
              "      <td>4.619100</td>\n",
              "      <td>4.917108</td>\n",
              "      <td>0.214804</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2850</td>\n",
              "      <td>4.658000</td>\n",
              "      <td>4.840864</td>\n",
              "      <td>0.230887</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3000</td>\n",
              "      <td>4.570500</td>\n",
              "      <td>4.721294</td>\n",
              "      <td>0.239255</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3150</td>\n",
              "      <td>4.140200</td>\n",
              "      <td>4.824236</td>\n",
              "      <td>0.256489</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3300</td>\n",
              "      <td>4.322100</td>\n",
              "      <td>4.707396</td>\n",
              "      <td>0.263081</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3450</td>\n",
              "      <td>4.016100</td>\n",
              "      <td>4.432916</td>\n",
              "      <td>0.242210</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3600</td>\n",
              "      <td>3.992500</td>\n",
              "      <td>4.549846</td>\n",
              "      <td>0.255887</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3750</td>\n",
              "      <td>4.523800</td>\n",
              "      <td>4.685084</td>\n",
              "      <td>0.246356</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3900</td>\n",
              "      <td>3.848200</td>\n",
              "      <td>4.451046</td>\n",
              "      <td>0.297258</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4050</td>\n",
              "      <td>3.921600</td>\n",
              "      <td>4.412374</td>\n",
              "      <td>0.285507</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4200</td>\n",
              "      <td>3.712400</td>\n",
              "      <td>4.479602</td>\n",
              "      <td>0.286944</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4350</td>\n",
              "      <td>4.543600</td>\n",
              "      <td>4.040593</td>\n",
              "      <td>0.337818</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4500</td>\n",
              "      <td>3.517700</td>\n",
              "      <td>4.238153</td>\n",
              "      <td>0.297468</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4650</td>\n",
              "      <td>4.183700</td>\n",
              "      <td>4.163253</td>\n",
              "      <td>0.321937</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4800</td>\n",
              "      <td>3.789500</td>\n",
              "      <td>3.993109</td>\n",
              "      <td>0.338733</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4950</td>\n",
              "      <td>4.429000</td>\n",
              "      <td>3.963912</td>\n",
              "      <td>0.328725</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5100</td>\n",
              "      <td>3.852500</td>\n",
              "      <td>4.258964</td>\n",
              "      <td>0.302937</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5250</td>\n",
              "      <td>3.687300</td>\n",
              "      <td>4.204965</td>\n",
              "      <td>0.296012</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5400</td>\n",
              "      <td>3.678500</td>\n",
              "      <td>4.113055</td>\n",
              "      <td>0.332406</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5550</td>\n",
              "      <td>3.576100</td>\n",
              "      <td>4.272637</td>\n",
              "      <td>0.329529</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5700</td>\n",
              "      <td>3.261900</td>\n",
              "      <td>4.377056</td>\n",
              "      <td>0.317647</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5850</td>\n",
              "      <td>3.865100</td>\n",
              "      <td>4.074553</td>\n",
              "      <td>0.369466</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6000</td>\n",
              "      <td>3.662700</td>\n",
              "      <td>4.199471</td>\n",
              "      <td>0.298887</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6150</td>\n",
              "      <td>4.263400</td>\n",
              "      <td>4.224988</td>\n",
              "      <td>0.302098</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6300</td>\n",
              "      <td>3.500300</td>\n",
              "      <td>3.900113</td>\n",
              "      <td>0.368116</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6450</td>\n",
              "      <td>3.942800</td>\n",
              "      <td>4.247036</td>\n",
              "      <td>0.317979</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6600</td>\n",
              "      <td>3.884000</td>\n",
              "      <td>3.657074</td>\n",
              "      <td>0.370787</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6750</td>\n",
              "      <td>3.475400</td>\n",
              "      <td>3.748548</td>\n",
              "      <td>0.385507</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6900</td>\n",
              "      <td>3.144300</td>\n",
              "      <td>3.871420</td>\n",
              "      <td>0.357585</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7050</td>\n",
              "      <td>4.155100</td>\n",
              "      <td>4.091904</td>\n",
              "      <td>0.315789</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7200</td>\n",
              "      <td>3.937700</td>\n",
              "      <td>3.797956</td>\n",
              "      <td>0.358936</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7350</td>\n",
              "      <td>3.594700</td>\n",
              "      <td>3.860197</td>\n",
              "      <td>0.363775</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7500</td>\n",
              "      <td>3.594700</td>\n",
              "      <td>3.664401</td>\n",
              "      <td>0.401225</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7650</td>\n",
              "      <td>3.721600</td>\n",
              "      <td>3.767460</td>\n",
              "      <td>0.377522</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7800</td>\n",
              "      <td>3.529700</td>\n",
              "      <td>4.064291</td>\n",
              "      <td>0.328612</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7950</td>\n",
              "      <td>3.912500</td>\n",
              "      <td>3.531082</td>\n",
              "      <td>0.361573</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8100</td>\n",
              "      <td>3.037300</td>\n",
              "      <td>3.775482</td>\n",
              "      <td>0.371720</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8250</td>\n",
              "      <td>3.053800</td>\n",
              "      <td>3.931973</td>\n",
              "      <td>0.328829</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8400</td>\n",
              "      <td>2.998000</td>\n",
              "      <td>3.804071</td>\n",
              "      <td>0.370427</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8550</td>\n",
              "      <td>3.319900</td>\n",
              "      <td>3.560786</td>\n",
              "      <td>0.394886</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8700</td>\n",
              "      <td>3.067900</td>\n",
              "      <td>3.492836</td>\n",
              "      <td>0.396114</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8850</td>\n",
              "      <td>3.912400</td>\n",
              "      <td>3.822231</td>\n",
              "      <td>0.368347</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9000</td>\n",
              "      <td>3.532800</td>\n",
              "      <td>3.867544</td>\n",
              "      <td>0.350829</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9150</td>\n",
              "      <td>3.233900</td>\n",
              "      <td>3.696751</td>\n",
              "      <td>0.381020</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9300</td>\n",
              "      <td>3.301900</td>\n",
              "      <td>3.625029</td>\n",
              "      <td>0.403008</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9450</td>\n",
              "      <td>3.825600</td>\n",
              "      <td>3.783301</td>\n",
              "      <td>0.381250</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9600</td>\n",
              "      <td>3.300900</td>\n",
              "      <td>3.767571</td>\n",
              "      <td>0.378766</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9750</td>\n",
              "      <td>3.012500</td>\n",
              "      <td>3.795917</td>\n",
              "      <td>0.389781</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9900</td>\n",
              "      <td>3.760700</td>\n",
              "      <td>3.422670</td>\n",
              "      <td>0.411189</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10050</td>\n",
              "      <td>3.247500</td>\n",
              "      <td>3.632880</td>\n",
              "      <td>0.362573</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10200</td>\n",
              "      <td>3.663700</td>\n",
              "      <td>3.679601</td>\n",
              "      <td>0.381872</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10350</td>\n",
              "      <td>3.718100</td>\n",
              "      <td>3.496844</td>\n",
              "      <td>0.383333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10500</td>\n",
              "      <td>3.240800</td>\n",
              "      <td>3.475301</td>\n",
              "      <td>0.406677</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10650</td>\n",
              "      <td>3.747400</td>\n",
              "      <td>3.605044</td>\n",
              "      <td>0.393007</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10800</td>\n",
              "      <td>3.262800</td>\n",
              "      <td>3.473817</td>\n",
              "      <td>0.400868</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10950</td>\n",
              "      <td>3.425400</td>\n",
              "      <td>3.813551</td>\n",
              "      <td>0.374449</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11100</td>\n",
              "      <td>3.123000</td>\n",
              "      <td>3.625914</td>\n",
              "      <td>0.374443</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11250</td>\n",
              "      <td>3.420200</td>\n",
              "      <td>3.596419</td>\n",
              "      <td>0.397988</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11400</td>\n",
              "      <td>3.501600</td>\n",
              "      <td>3.799321</td>\n",
              "      <td>0.393665</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11550</td>\n",
              "      <td>3.797300</td>\n",
              "      <td>3.554082</td>\n",
              "      <td>0.405882</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11700</td>\n",
              "      <td>2.796300</td>\n",
              "      <td>3.689616</td>\n",
              "      <td>0.375552</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11850</td>\n",
              "      <td>3.195600</td>\n",
              "      <td>3.730153</td>\n",
              "      <td>0.387879</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "tokenizer_name = \"bert-base-uncased\"\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"h-saute-mlm-76m-0.0.1\",\n",
        "    eval_strategy=\"steps\",\n",
        "    eval_steps=150,\n",
        "    per_device_train_batch_size=1,\n",
        "    per_device_eval_batch_size=1,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    save_strategy=\"epoch\",\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_steps=150,\n",
        "    fp16=True,\n",
        "    max_steps=1506100\n",
        "    # deepspeed=\"deepspeed_config.json\",  # optional\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    tokenizer=None,\n",
        "    data_collator=lambda batch: batch[0],\n",
        "    callbacks=[wandb_logger_callback],\n",
        "    eval_dataset=test_dataset,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "semnE50P-wHO"
      },
      "source": [
        "### Bert Baseline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XJ1BCm4l-wHP"
      },
      "source": [
        "#### Load Model and dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "yVxvxh4q-wHP"
      },
      "outputs": [],
      "source": [
        "train_dataset = SAUTEDataset(split=\"train\", dialog_format=\"full\")\n",
        "eval_dataset = SAUTEDataset(split=\"test\", dialog_format=\"full\")\n",
        "\n",
        "from torch.utils.data import Subset\n",
        "import random\n",
        "\n",
        "subset_size = 50\n",
        "indices = random.sample(range(len(eval_dataset)), subset_size)\n",
        "test_dataset = Subset(eval_dataset, indices)\n",
        "\n",
        "bert_config = BertConfig(\n",
        "    vocab_size=30522,\n",
        "    hidden_size=768,\n",
        "    num_hidden_layers=6,\n",
        "    num_attention_heads=12,\n",
        "    intermediate_size=3072,\n",
        "    max_position_embeddings=512,\n",
        "    hidden_dropout_prob=0.1,\n",
        "    attention_probs_dropout_prob=0.1\n",
        ")\n",
        "model = BertForMaskedLM(config=bert_config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "C3rHnZgw-wHQ"
      },
      "outputs": [],
      "source": [
        "fixed_batch = train_dataset[1]\n",
        "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "wandb_logger_callback = WandbPredictionLoggerCallback(\n",
        "    fixed_batch=fixed_batch,\n",
        "    tokenizer=tokenizer,\n",
        "    log_every_steps=10\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Model Statistics"
      ],
      "metadata": {
        "id": "-5cfqxBE4uWd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "summary(\n",
        "    model,\n",
        "    input_data=(fixed_batch[\"input_ids\"], fixed_batch[\"attention_mask\"]),\n",
        "    col_names=[\"input_size\", \"output_size\", \"num_params\", \"mult_adds\"],\n",
        "    depth=3,\n",
        "    verbose=1\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ybB0xHep4vyb",
        "outputId": "bd7e1a70-bfc9-4988-b4fc-78f9b202fb49"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================================================================================================\n",
            "Layer (type:depth-idx)                                       Input Shape               Output Shape              Param #                   Mult-Adds\n",
            "================================================================================================================================================================\n",
            "BertForMaskedLM                                              [1, 128]                  [1, 128, 30522]           --                        --\n",
            "├─BertModel: 1-1                                             [1, 128]                  [1, 128, 768]             --                        --\n",
            "│    └─BertEmbeddings: 2-1                                   --                        [1, 128, 768]             --                        --\n",
            "│    │    └─Embedding: 3-1                                   [1, 128]                  [1, 128, 768]             23,440,896                23,440,896\n",
            "│    │    └─Embedding: 3-2                                   [1, 128]                  [1, 128, 768]             1,536                     1,536\n",
            "│    │    └─Embedding: 3-3                                   [1, 128]                  [1, 128, 768]             393,216                   393,216\n",
            "│    │    └─LayerNorm: 3-4                                   [1, 128, 768]             [1, 128, 768]             1,536                     1,536\n",
            "│    │    └─Dropout: 3-5                                     [1, 128, 768]             [1, 128, 768]             --                        --\n",
            "│    └─BertEncoder: 2-2                                      [1, 128, 768]             [1, 128, 768]             --                        --\n",
            "│    │    └─ModuleList: 3-6                                  --                        --                        42,527,232                --\n",
            "├─BertOnlyMLMHead: 1-2                                       [1, 128, 768]             [1, 128, 30522]           --                        --\n",
            "│    └─BertLMPredictionHead: 2-3                             [1, 128, 768]             [1, 128, 30522]           --                        --\n",
            "│    │    └─BertPredictionHeadTransform: 3-7                 [1, 128, 768]             [1, 128, 768]             592,128                   592,128\n",
            "│    │    └─Linear: 3-8                                      [1, 128, 768]             [1, 128, 30522]           23,471,418                23,471,418\n",
            "================================================================================================================================================================\n",
            "Total params: 90,427,962\n",
            "Trainable params: 90,427,962\n",
            "Non-trainable params: 0\n",
            "Total mult-adds (Units.MEGABYTES): 90.43\n",
            "================================================================================================================================================================\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 87.88\n",
            "Params size (MB): 361.71\n",
            "Estimated Total Size (MB): 449.59\n",
            "================================================================================================================================================================\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "================================================================================================================================================================\n",
              "Layer (type:depth-idx)                                       Input Shape               Output Shape              Param #                   Mult-Adds\n",
              "================================================================================================================================================================\n",
              "BertForMaskedLM                                              [1, 128]                  [1, 128, 30522]           --                        --\n",
              "├─BertModel: 1-1                                             [1, 128]                  [1, 128, 768]             --                        --\n",
              "│    └─BertEmbeddings: 2-1                                   --                        [1, 128, 768]             --                        --\n",
              "│    │    └─Embedding: 3-1                                   [1, 128]                  [1, 128, 768]             23,440,896                23,440,896\n",
              "│    │    └─Embedding: 3-2                                   [1, 128]                  [1, 128, 768]             1,536                     1,536\n",
              "│    │    └─Embedding: 3-3                                   [1, 128]                  [1, 128, 768]             393,216                   393,216\n",
              "│    │    └─LayerNorm: 3-4                                   [1, 128, 768]             [1, 128, 768]             1,536                     1,536\n",
              "│    │    └─Dropout: 3-5                                     [1, 128, 768]             [1, 128, 768]             --                        --\n",
              "│    └─BertEncoder: 2-2                                      [1, 128, 768]             [1, 128, 768]             --                        --\n",
              "│    │    └─ModuleList: 3-6                                  --                        --                        42,527,232                --\n",
              "├─BertOnlyMLMHead: 1-2                                       [1, 128, 768]             [1, 128, 30522]           --                        --\n",
              "│    └─BertLMPredictionHead: 2-3                             [1, 128, 768]             [1, 128, 30522]           --                        --\n",
              "│    │    └─BertPredictionHeadTransform: 3-7                 [1, 128, 768]             [1, 128, 768]             592,128                   592,128\n",
              "│    │    └─Linear: 3-8                                      [1, 128, 768]             [1, 128, 30522]           23,471,418                23,471,418\n",
              "================================================================================================================================================================\n",
              "Total params: 90,427,962\n",
              "Trainable params: 90,427,962\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (Units.MEGABYTES): 90.43\n",
              "================================================================================================================================================================\n",
              "Input size (MB): 0.00\n",
              "Forward/backward pass size (MB): 87.88\n",
              "Params size (MB): 361.71\n",
              "Estimated Total Size (MB): 449.59\n",
              "================================================================================================================================================================"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e3BfmM1K-wHQ"
      },
      "source": [
        "#### Train model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "1KH7_eN8-wHR",
        "outputId": "5e188cd7-df81-4cc3-f630-fb47a3668152"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjustinduc\u001b[0m (\u001b[33mjustinduc-epitech\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.9"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250420_075108-79j3uk5r</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/justinduc-epitech/huggingface/runs/79j3uk5r' target=\"_blank\">bert-baseline-mlm-90m</a></strong> to <a href='https://wandb.ai/justinduc-epitech/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/justinduc-epitech/huggingface' target=\"_blank\">https://wandb.ai/justinduc-epitech/huggingface</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/justinduc-epitech/huggingface/runs/79j3uk5r' target=\"_blank\">https://wandb.ai/justinduc-epitech/huggingface/runs/79j3uk5r</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='10196' max='3574746' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [  10196/3574746 15:15 < 88:52:38, 11.14 it/s, Epoch 0.01/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Masked Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>7.283000</td>\n",
              "      <td>6.017062</td>\n",
              "      <td>0.045455</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>5.911900</td>\n",
              "      <td>5.686316</td>\n",
              "      <td>0.115385</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>450</td>\n",
              "      <td>5.646800</td>\n",
              "      <td>5.427762</td>\n",
              "      <td>0.114035</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>5.798500</td>\n",
              "      <td>5.420510</td>\n",
              "      <td>0.144000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>750</td>\n",
              "      <td>5.598300</td>\n",
              "      <td>5.362011</td>\n",
              "      <td>0.117241</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>5.614700</td>\n",
              "      <td>5.330958</td>\n",
              "      <td>0.190141</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1050</td>\n",
              "      <td>5.530500</td>\n",
              "      <td>5.254026</td>\n",
              "      <td>0.104839</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1200</td>\n",
              "      <td>5.576400</td>\n",
              "      <td>5.606144</td>\n",
              "      <td>0.126866</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1350</td>\n",
              "      <td>5.411200</td>\n",
              "      <td>5.772059</td>\n",
              "      <td>0.128378</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>5.525300</td>\n",
              "      <td>5.278173</td>\n",
              "      <td>0.172414</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1650</td>\n",
              "      <td>5.439900</td>\n",
              "      <td>4.976476</td>\n",
              "      <td>0.152318</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1800</td>\n",
              "      <td>5.455000</td>\n",
              "      <td>4.879007</td>\n",
              "      <td>0.185714</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1950</td>\n",
              "      <td>5.468900</td>\n",
              "      <td>5.355076</td>\n",
              "      <td>0.139706</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2100</td>\n",
              "      <td>5.262400</td>\n",
              "      <td>5.260105</td>\n",
              "      <td>0.144737</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2250</td>\n",
              "      <td>5.470200</td>\n",
              "      <td>5.150380</td>\n",
              "      <td>0.117241</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2400</td>\n",
              "      <td>5.471900</td>\n",
              "      <td>5.139132</td>\n",
              "      <td>0.180000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2550</td>\n",
              "      <td>5.365600</td>\n",
              "      <td>5.041901</td>\n",
              "      <td>0.105634</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2700</td>\n",
              "      <td>5.325200</td>\n",
              "      <td>4.737654</td>\n",
              "      <td>0.165517</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2850</td>\n",
              "      <td>5.439000</td>\n",
              "      <td>5.223706</td>\n",
              "      <td>0.168224</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3000</td>\n",
              "      <td>5.329200</td>\n",
              "      <td>5.220749</td>\n",
              "      <td>0.163121</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3150</td>\n",
              "      <td>5.360100</td>\n",
              "      <td>5.055959</td>\n",
              "      <td>0.134752</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3300</td>\n",
              "      <td>5.283900</td>\n",
              "      <td>5.096445</td>\n",
              "      <td>0.152174</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3450</td>\n",
              "      <td>5.294600</td>\n",
              "      <td>5.140679</td>\n",
              "      <td>0.146552</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3600</td>\n",
              "      <td>5.296300</td>\n",
              "      <td>5.315488</td>\n",
              "      <td>0.100840</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3750</td>\n",
              "      <td>5.304600</td>\n",
              "      <td>4.809178</td>\n",
              "      <td>0.139535</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3900</td>\n",
              "      <td>5.369000</td>\n",
              "      <td>5.001616</td>\n",
              "      <td>0.126984</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4050</td>\n",
              "      <td>5.345700</td>\n",
              "      <td>5.057741</td>\n",
              "      <td>0.181818</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4200</td>\n",
              "      <td>5.308000</td>\n",
              "      <td>4.896286</td>\n",
              "      <td>0.139130</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4350</td>\n",
              "      <td>5.382900</td>\n",
              "      <td>5.024888</td>\n",
              "      <td>0.172932</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4500</td>\n",
              "      <td>5.352000</td>\n",
              "      <td>5.070847</td>\n",
              "      <td>0.168142</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4650</td>\n",
              "      <td>5.243100</td>\n",
              "      <td>4.954284</td>\n",
              "      <td>0.167883</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4800</td>\n",
              "      <td>5.236000</td>\n",
              "      <td>5.050357</td>\n",
              "      <td>0.149351</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4950</td>\n",
              "      <td>5.252100</td>\n",
              "      <td>4.837687</td>\n",
              "      <td>0.198473</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5100</td>\n",
              "      <td>5.366900</td>\n",
              "      <td>5.327510</td>\n",
              "      <td>0.174603</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5250</td>\n",
              "      <td>5.352100</td>\n",
              "      <td>5.286411</td>\n",
              "      <td>0.095588</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5400</td>\n",
              "      <td>5.378200</td>\n",
              "      <td>5.180659</td>\n",
              "      <td>0.181818</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5550</td>\n",
              "      <td>5.209200</td>\n",
              "      <td>5.146042</td>\n",
              "      <td>0.140496</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5700</td>\n",
              "      <td>5.298100</td>\n",
              "      <td>4.986480</td>\n",
              "      <td>0.155738</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5850</td>\n",
              "      <td>5.247500</td>\n",
              "      <td>5.284936</td>\n",
              "      <td>0.172662</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6000</td>\n",
              "      <td>5.380300</td>\n",
              "      <td>5.315496</td>\n",
              "      <td>0.119718</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6150</td>\n",
              "      <td>5.195900</td>\n",
              "      <td>4.771910</td>\n",
              "      <td>0.220000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6300</td>\n",
              "      <td>5.205200</td>\n",
              "      <td>5.080381</td>\n",
              "      <td>0.181818</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6450</td>\n",
              "      <td>5.280600</td>\n",
              "      <td>4.691129</td>\n",
              "      <td>0.158621</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6600</td>\n",
              "      <td>5.242000</td>\n",
              "      <td>4.977095</td>\n",
              "      <td>0.165517</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6750</td>\n",
              "      <td>5.241500</td>\n",
              "      <td>5.154130</td>\n",
              "      <td>0.171642</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6900</td>\n",
              "      <td>5.228200</td>\n",
              "      <td>5.210092</td>\n",
              "      <td>0.165517</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7050</td>\n",
              "      <td>5.287400</td>\n",
              "      <td>5.007958</td>\n",
              "      <td>0.134921</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7200</td>\n",
              "      <td>5.137100</td>\n",
              "      <td>5.437459</td>\n",
              "      <td>0.166667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7350</td>\n",
              "      <td>5.163000</td>\n",
              "      <td>5.010133</td>\n",
              "      <td>0.147287</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7500</td>\n",
              "      <td>5.246500</td>\n",
              "      <td>5.025325</td>\n",
              "      <td>0.172662</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7650</td>\n",
              "      <td>5.244100</td>\n",
              "      <td>4.914804</td>\n",
              "      <td>0.136364</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7800</td>\n",
              "      <td>5.093200</td>\n",
              "      <td>4.724351</td>\n",
              "      <td>0.206349</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7950</td>\n",
              "      <td>5.295000</td>\n",
              "      <td>4.942934</td>\n",
              "      <td>0.166667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8100</td>\n",
              "      <td>5.341000</td>\n",
              "      <td>5.009624</td>\n",
              "      <td>0.113475</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8250</td>\n",
              "      <td>5.038500</td>\n",
              "      <td>5.031781</td>\n",
              "      <td>0.110345</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8400</td>\n",
              "      <td>5.155000</td>\n",
              "      <td>5.355800</td>\n",
              "      <td>0.159664</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8550</td>\n",
              "      <td>5.260300</td>\n",
              "      <td>5.285225</td>\n",
              "      <td>0.140845</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8700</td>\n",
              "      <td>5.080500</td>\n",
              "      <td>4.787586</td>\n",
              "      <td>0.183824</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8850</td>\n",
              "      <td>5.134900</td>\n",
              "      <td>4.602723</td>\n",
              "      <td>0.178571</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9000</td>\n",
              "      <td>5.122200</td>\n",
              "      <td>4.681954</td>\n",
              "      <td>0.200000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9150</td>\n",
              "      <td>5.121200</td>\n",
              "      <td>4.765173</td>\n",
              "      <td>0.171053</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9300</td>\n",
              "      <td>5.160700</td>\n",
              "      <td>5.533111</td>\n",
              "      <td>0.184000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9450</td>\n",
              "      <td>5.196200</td>\n",
              "      <td>4.915768</td>\n",
              "      <td>0.166667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9600</td>\n",
              "      <td>5.151800</td>\n",
              "      <td>4.733032</td>\n",
              "      <td>0.189781</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9750</td>\n",
              "      <td>5.236100</td>\n",
              "      <td>5.109291</td>\n",
              "      <td>0.157480</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9900</td>\n",
              "      <td>5.144600</td>\n",
              "      <td>5.178728</td>\n",
              "      <td>0.148148</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10050</td>\n",
              "      <td>5.246600</td>\n",
              "      <td>4.961353</td>\n",
              "      <td>0.154412</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='13428' max='3574746' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [  13428/3574746 20:09 < 89:06:24, 11.10 it/s, Epoch 0.01/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Masked Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>7.283000</td>\n",
              "      <td>6.017062</td>\n",
              "      <td>0.045455</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>5.911900</td>\n",
              "      <td>5.686316</td>\n",
              "      <td>0.115385</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>450</td>\n",
              "      <td>5.646800</td>\n",
              "      <td>5.427762</td>\n",
              "      <td>0.114035</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>5.798500</td>\n",
              "      <td>5.420510</td>\n",
              "      <td>0.144000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>750</td>\n",
              "      <td>5.598300</td>\n",
              "      <td>5.362011</td>\n",
              "      <td>0.117241</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>5.614700</td>\n",
              "      <td>5.330958</td>\n",
              "      <td>0.190141</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1050</td>\n",
              "      <td>5.530500</td>\n",
              "      <td>5.254026</td>\n",
              "      <td>0.104839</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1200</td>\n",
              "      <td>5.576400</td>\n",
              "      <td>5.606144</td>\n",
              "      <td>0.126866</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1350</td>\n",
              "      <td>5.411200</td>\n",
              "      <td>5.772059</td>\n",
              "      <td>0.128378</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>5.525300</td>\n",
              "      <td>5.278173</td>\n",
              "      <td>0.172414</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1650</td>\n",
              "      <td>5.439900</td>\n",
              "      <td>4.976476</td>\n",
              "      <td>0.152318</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1800</td>\n",
              "      <td>5.455000</td>\n",
              "      <td>4.879007</td>\n",
              "      <td>0.185714</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1950</td>\n",
              "      <td>5.468900</td>\n",
              "      <td>5.355076</td>\n",
              "      <td>0.139706</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2100</td>\n",
              "      <td>5.262400</td>\n",
              "      <td>5.260105</td>\n",
              "      <td>0.144737</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2250</td>\n",
              "      <td>5.470200</td>\n",
              "      <td>5.150380</td>\n",
              "      <td>0.117241</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2400</td>\n",
              "      <td>5.471900</td>\n",
              "      <td>5.139132</td>\n",
              "      <td>0.180000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2550</td>\n",
              "      <td>5.365600</td>\n",
              "      <td>5.041901</td>\n",
              "      <td>0.105634</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2700</td>\n",
              "      <td>5.325200</td>\n",
              "      <td>4.737654</td>\n",
              "      <td>0.165517</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2850</td>\n",
              "      <td>5.439000</td>\n",
              "      <td>5.223706</td>\n",
              "      <td>0.168224</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3000</td>\n",
              "      <td>5.329200</td>\n",
              "      <td>5.220749</td>\n",
              "      <td>0.163121</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3150</td>\n",
              "      <td>5.360100</td>\n",
              "      <td>5.055959</td>\n",
              "      <td>0.134752</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3300</td>\n",
              "      <td>5.283900</td>\n",
              "      <td>5.096445</td>\n",
              "      <td>0.152174</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3450</td>\n",
              "      <td>5.294600</td>\n",
              "      <td>5.140679</td>\n",
              "      <td>0.146552</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3600</td>\n",
              "      <td>5.296300</td>\n",
              "      <td>5.315488</td>\n",
              "      <td>0.100840</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3750</td>\n",
              "      <td>5.304600</td>\n",
              "      <td>4.809178</td>\n",
              "      <td>0.139535</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3900</td>\n",
              "      <td>5.369000</td>\n",
              "      <td>5.001616</td>\n",
              "      <td>0.126984</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4050</td>\n",
              "      <td>5.345700</td>\n",
              "      <td>5.057741</td>\n",
              "      <td>0.181818</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4200</td>\n",
              "      <td>5.308000</td>\n",
              "      <td>4.896286</td>\n",
              "      <td>0.139130</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4350</td>\n",
              "      <td>5.382900</td>\n",
              "      <td>5.024888</td>\n",
              "      <td>0.172932</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4500</td>\n",
              "      <td>5.352000</td>\n",
              "      <td>5.070847</td>\n",
              "      <td>0.168142</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4650</td>\n",
              "      <td>5.243100</td>\n",
              "      <td>4.954284</td>\n",
              "      <td>0.167883</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4800</td>\n",
              "      <td>5.236000</td>\n",
              "      <td>5.050357</td>\n",
              "      <td>0.149351</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4950</td>\n",
              "      <td>5.252100</td>\n",
              "      <td>4.837687</td>\n",
              "      <td>0.198473</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5100</td>\n",
              "      <td>5.366900</td>\n",
              "      <td>5.327510</td>\n",
              "      <td>0.174603</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5250</td>\n",
              "      <td>5.352100</td>\n",
              "      <td>5.286411</td>\n",
              "      <td>0.095588</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5400</td>\n",
              "      <td>5.378200</td>\n",
              "      <td>5.180659</td>\n",
              "      <td>0.181818</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5550</td>\n",
              "      <td>5.209200</td>\n",
              "      <td>5.146042</td>\n",
              "      <td>0.140496</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5700</td>\n",
              "      <td>5.298100</td>\n",
              "      <td>4.986480</td>\n",
              "      <td>0.155738</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5850</td>\n",
              "      <td>5.247500</td>\n",
              "      <td>5.284936</td>\n",
              "      <td>0.172662</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6000</td>\n",
              "      <td>5.380300</td>\n",
              "      <td>5.315496</td>\n",
              "      <td>0.119718</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6150</td>\n",
              "      <td>5.195900</td>\n",
              "      <td>4.771910</td>\n",
              "      <td>0.220000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6300</td>\n",
              "      <td>5.205200</td>\n",
              "      <td>5.080381</td>\n",
              "      <td>0.181818</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6450</td>\n",
              "      <td>5.280600</td>\n",
              "      <td>4.691129</td>\n",
              "      <td>0.158621</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6600</td>\n",
              "      <td>5.242000</td>\n",
              "      <td>4.977095</td>\n",
              "      <td>0.165517</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6750</td>\n",
              "      <td>5.241500</td>\n",
              "      <td>5.154130</td>\n",
              "      <td>0.171642</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6900</td>\n",
              "      <td>5.228200</td>\n",
              "      <td>5.210092</td>\n",
              "      <td>0.165517</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7050</td>\n",
              "      <td>5.287400</td>\n",
              "      <td>5.007958</td>\n",
              "      <td>0.134921</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7200</td>\n",
              "      <td>5.137100</td>\n",
              "      <td>5.437459</td>\n",
              "      <td>0.166667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7350</td>\n",
              "      <td>5.163000</td>\n",
              "      <td>5.010133</td>\n",
              "      <td>0.147287</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7500</td>\n",
              "      <td>5.246500</td>\n",
              "      <td>5.025325</td>\n",
              "      <td>0.172662</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7650</td>\n",
              "      <td>5.244100</td>\n",
              "      <td>4.914804</td>\n",
              "      <td>0.136364</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7800</td>\n",
              "      <td>5.093200</td>\n",
              "      <td>4.724351</td>\n",
              "      <td>0.206349</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7950</td>\n",
              "      <td>5.295000</td>\n",
              "      <td>4.942934</td>\n",
              "      <td>0.166667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8100</td>\n",
              "      <td>5.341000</td>\n",
              "      <td>5.009624</td>\n",
              "      <td>0.113475</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8250</td>\n",
              "      <td>5.038500</td>\n",
              "      <td>5.031781</td>\n",
              "      <td>0.110345</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8400</td>\n",
              "      <td>5.155000</td>\n",
              "      <td>5.355800</td>\n",
              "      <td>0.159664</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8550</td>\n",
              "      <td>5.260300</td>\n",
              "      <td>5.285225</td>\n",
              "      <td>0.140845</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8700</td>\n",
              "      <td>5.080500</td>\n",
              "      <td>4.787586</td>\n",
              "      <td>0.183824</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8850</td>\n",
              "      <td>5.134900</td>\n",
              "      <td>4.602723</td>\n",
              "      <td>0.178571</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9000</td>\n",
              "      <td>5.122200</td>\n",
              "      <td>4.681954</td>\n",
              "      <td>0.200000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9150</td>\n",
              "      <td>5.121200</td>\n",
              "      <td>4.765173</td>\n",
              "      <td>0.171053</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9300</td>\n",
              "      <td>5.160700</td>\n",
              "      <td>5.533111</td>\n",
              "      <td>0.184000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9450</td>\n",
              "      <td>5.196200</td>\n",
              "      <td>4.915768</td>\n",
              "      <td>0.166667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9600</td>\n",
              "      <td>5.151800</td>\n",
              "      <td>4.733032</td>\n",
              "      <td>0.189781</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9750</td>\n",
              "      <td>5.236100</td>\n",
              "      <td>5.109291</td>\n",
              "      <td>0.157480</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9900</td>\n",
              "      <td>5.144600</td>\n",
              "      <td>5.178728</td>\n",
              "      <td>0.148148</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10050</td>\n",
              "      <td>5.246600</td>\n",
              "      <td>4.961353</td>\n",
              "      <td>0.154412</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10200</td>\n",
              "      <td>5.187500</td>\n",
              "      <td>5.062796</td>\n",
              "      <td>0.171171</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10350</td>\n",
              "      <td>5.181900</td>\n",
              "      <td>5.112452</td>\n",
              "      <td>0.195489</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10500</td>\n",
              "      <td>5.235100</td>\n",
              "      <td>5.428340</td>\n",
              "      <td>0.184615</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10650</td>\n",
              "      <td>5.162400</td>\n",
              "      <td>4.844765</td>\n",
              "      <td>0.173554</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10800</td>\n",
              "      <td>5.076100</td>\n",
              "      <td>5.448974</td>\n",
              "      <td>0.167939</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10950</td>\n",
              "      <td>5.164400</td>\n",
              "      <td>4.974771</td>\n",
              "      <td>0.144928</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11100</td>\n",
              "      <td>5.081000</td>\n",
              "      <td>4.803138</td>\n",
              "      <td>0.182482</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11250</td>\n",
              "      <td>5.097100</td>\n",
              "      <td>5.457529</td>\n",
              "      <td>0.180328</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11400</td>\n",
              "      <td>5.260800</td>\n",
              "      <td>5.089576</td>\n",
              "      <td>0.185185</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11550</td>\n",
              "      <td>5.241100</td>\n",
              "      <td>4.796432</td>\n",
              "      <td>0.212766</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11700</td>\n",
              "      <td>5.133000</td>\n",
              "      <td>5.100833</td>\n",
              "      <td>0.151515</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11850</td>\n",
              "      <td>5.134700</td>\n",
              "      <td>4.912872</td>\n",
              "      <td>0.160000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12000</td>\n",
              "      <td>5.080000</td>\n",
              "      <td>4.838019</td>\n",
              "      <td>0.197183</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12150</td>\n",
              "      <td>5.129100</td>\n",
              "      <td>5.122423</td>\n",
              "      <td>0.147541</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12300</td>\n",
              "      <td>5.121300</td>\n",
              "      <td>4.822365</td>\n",
              "      <td>0.188406</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12450</td>\n",
              "      <td>4.988400</td>\n",
              "      <td>5.112532</td>\n",
              "      <td>0.192000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12600</td>\n",
              "      <td>5.225700</td>\n",
              "      <td>5.106948</td>\n",
              "      <td>0.164062</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12750</td>\n",
              "      <td>5.211900</td>\n",
              "      <td>4.569066</td>\n",
              "      <td>0.246154</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12900</td>\n",
              "      <td>5.251300</td>\n",
              "      <td>5.081385</td>\n",
              "      <td>0.136364</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13050</td>\n",
              "      <td>5.120900</td>\n",
              "      <td>4.790004</td>\n",
              "      <td>0.169643</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13200</td>\n",
              "      <td>5.255300</td>\n",
              "      <td>5.030198</td>\n",
              "      <td>0.135338</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13350</td>\n",
              "      <td>5.141200</td>\n",
              "      <td>4.801039</td>\n",
              "      <td>0.176923</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"bert-baseline-mlm-90m\",\n",
        "    per_device_train_batch_size=1,\n",
        "    # save_strategy=\"steps\",\n",
        "    # save_steps=1000,\n",
        "    eval_strategy=\"steps\",\n",
        "    eval_steps=150,\n",
        "    logging_steps=150,\n",
        "    learning_rate=5e-5,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    report_to=\"wandb\",\n",
        ")\n",
        "\n",
        "# Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset,\n",
        "    data_collator=lambda batch: batch[0],\n",
        "    callbacks=[wandb_logger_callback],\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "trainer.train()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}